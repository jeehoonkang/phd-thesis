\section{Background: A Brief Tour of CompCert}
\label{sec:background}

We begin by briefly reviewing the technical background that informs the rest of this dissertation,
using CompCert as the learning material.  We first explain CompCert's correctness statement, as well
as its simulation verification method (\Cref{sec:background:correctness}).  To flesh out the details
on formal semantics and compiler verification, we use constant propagation---one of CompCert's
optimizations---as a running example.  Specifically, We review the RTL language on which constant
propagation is performed (\Cref{sec:background:rtl}) and CompCert's memory model that is designed to
verify compiler optimizations (\Cref{sec:background:memory}), and explain how constant propagation
works and how CompCert verifies it (\Cref{sec:background:constprop}).  Throughout this section, we
keep the presentation semi-formal, abstracting away unnecessary detail to get across the main ideas.


\subsection{Compiler Correctness}
\label{sec:background:correctness}

\paragraph{End-to-End Correctness}

Roughly speaking, the correctness result of CompCert can be understood to assert the following.
Suppose \code{s.c} is a ``source'' file (in C), \code{t.asm} is a ``target'' file (in assembly), and
$\mathcal{C}$ is a verified compiler (represented as a function from C files to assembly files).
\[
\frac{
\mathcal{C}(\mathtt{s.c}) = \mathtt{t.asm} \qquad
s = \mathrm{load}(\mathtt{s.c})\qquad
t = \mathrm{load}(\mathtt{t.asm})}
{\mathrm{Behav}(s) \supseteq \mathrm{Behav}(t)}
\]
If \code{t.asm} is the result of compiling \code{s.c} with $\mathcal{C}$, then executing
\code{t.asm} according to assembly semantics will result in a subset of the behaviors one could
observe from executing \code{s.c} according to C semantics.  (We write
$s = \mathrm{load}(\mathtt{s.c})$ to denote the \emph{machine state} that results from loading
$\mathtt{s.c}$ into memory, $\mathrm{Behav}(s)$ to denote the observable behaviors of the execution
of $s$, and analogously for $t$ and $\mathtt{t.asm}$.)  Hence, we say that \code{t.asm}, the
target-level output of $\mathcal{C}$, \emph{refines} its source-level input, \code{s.c}.


\paragraph{Set of Behaviors}

% The standard notion of compiler correctness is behavioral refinement, which states that \emph{the
%   set of target program behaviors must be a subset of the set of source program behaviors}.
We consider sets of behaviors as opposed to single behaviors because a program may produce multiple
behaviors due to nondeterminism.  Given a set of I/O events that programs may generate and users may
observe, a behavior is one of the following three forms: $(1)$ a terminating execution producing a
finite sequence of I/O events, $e_1, \cdots, e_n, \mathtt{term}$; $(2)$ a diverging execution that
has produced only a finite sequence of I/O events, $e_1, \cdots, e_n, \mathtt{nonterm}$; and $(3)$ a
diverging execution producing an infinite sequence of I/O events, $e_1, \cdots, e_n, \cdots$.

Undefined behavior requires special attention in defining the set of behaviors, because the program
states in the condition are neither terminated nor transitioning to other states, fitting into none
of the behavioral categories.  Thus we assign \emph{the set of all behaviors} to the program states
invoking undefined behavior in order to validate compiler optimizations: if the target program's
behavior is undefined, then the source program's behavior should also be undefined; on the other
hand, if the source program's behavior is undefined, then compiler can choose any program as its
result.
% We regard \emph{the undefined behavior as described in the ISO C standards as the set of all
% behaviors}.  This captures the intuitive properties of compilers on undefined behavior.


\paragraph{Per-Pass Correctness}

\begin{figure}[!t]
\begin{center}
\includegraphics[width=\textwidth]{compcert-diagram.png}
\end{center}
\caption{Organization of CompCert (from~\cite{compcert-web})  \jeehoon{too old, change it}}
\label{fig:compcert-organization}
\end{figure}

To verify compilation correctness for the compiler $\mathcal{C}$, we verify each pass of
$\mathcal{C}$ independently.  Specifically, for each pass (transformation) $\mathcal{T}$ from
language $L_1$ to $L_2$---where the $L_i$'s may be C, assembly, or some intermediate languages---we
show the following:
\[
\frac{
\mathcal{T}(\mathtt{s.l1}) = \mathtt{t.l2} \qquad
s = \mathrm{load}(\mathtt{s.l1}) \qquad
t = \mathrm{load}(\mathtt{t.l2})
}
{
\mathrm{Behav}(s) \supseteq \mathrm{Behav}(t)
}
\]
That is, given the input \code{s.l1} and output \code{t.l2} of the $\mathcal{T}$ transformation, we
show that the behaviors of \code{t.l2} are contained within those of \code{s.l1}.  Since subset
inclusion is transitive, it easy to see that the proofs of the constituent passes of $\mathcal{C}$
compose to establish the correctness of $\mathcal{C}$ as a whole.

It is worth noting that CompCert consists of around a dozen optimization and transformation passes
going through quite a few intermediate languages, as illustrated in
\Cref{fig:compcert-organization}, and mainstream compilers typically have dozens of passes.


\paragraph{Verifying Per-Pass Correctness}

Now how does one actually prove the verification condition for each individual pass?  The standard
approach taken by CompCert is to use (closed) simulations.  Informally, we will say that a
\emph{simulation} $R$ is a relation between running programs (\ie machine states) in $L_1$ and $L_2$
such that, if $(s,t) \in R$, then the behaviors one observes while stepping through the execution of
$t$ are matched by corresponding observable behaviors in the execution of $s$.  One can think of $R$
as imposing an invariant, which describes (and connects) the possible machine states of the source
and target programs, and which must be maintained as the programs execute.  We leave further details
about simulations until later in the paper; suffice it to say that they satisfy the following
``adequacy'' property:
% , and we show that the machine states arising from
% loading \code{s.l1} and \code{t.l2} belong to this simulation.
\[
\frac{
R~\mbox{is a simulation} \qquad
(s,t)\in R
}{
\mathrm{Behav}(s) \supseteq \mathrm{Behav}(t)
}
\]
Thus, to establish the verification condition for pass $\mathcal{T}$, it suffices to exhibit a
simulation $R$ that relates $\mathrm{load}(\mathtt{s.l1})$ and $\mathrm{load}(\mathtt{t.l2})$.

\textbf{Note:} The verification approach described above, relying on ``backward'' simulations, is
something of an oversimplification of what CompCert actually does.  In fact, to make the proofs more
convenient, CompCert uses a mixture of forward and backward simulations.  We gloss over this point
here because it is orthogonal to our story, but we will return to it in \Cref{chap:sepcomp}.




\subsection{The RTL Language}
\label{sec:background:rtl}

\jeehoon{We're beginning with C, but here we're discussing RTL.  Is it okay?}

In the rest of this section, we will explain how to establish a simulation relation for optimization
passes, using constant propagation as a running example.  We start with the syntax and semantics of
CompCert's register transfer language (RTL), the compiler's internal language where constant
propagation takes place.  For presentation purposes, we simplify the language a bit by removing
types and other unnecessary details.


\paragraph{Syntax}

\input{rtl-syntax}

The syntax of the CompCert's RTL is given in Figure~\ref{fig:rtl-syntax}.  Programs are just a list
of global declarations, which consist of $(1)$ declarations of external variables and functions
provided by different compilation units, and $(2)$ definitions of variables and functions provided
by the current compilation unit.  For global variable declarations and definitions, we also specify
a (positive) integer number denoting the size of the declared block in bytes.

Function declarations only contain the function signature, which is a list of parameters, but
function definitions additionally contain a list of local registers, the size of their stack frame,
and the code.  The code is essentially a control-flow graph of three-address code: it is represented
as a mapping from node identifiers to instructions, where instructions either
%
do some local computation (\eg write a constant to a register, or perform some arithmetic
computation),
%
load from a memory address, 
%
store to memory,
%
do a comparison,
%
call a function, 
%
or exit the function and return a result. 
%
Each instruction also stores the node identifier(s) of its successor instruction(s).

Throughout we assume that programs satisfy some basic well-formedness properties: there cannot be
multiple definitions for the same global variable, declarations and definitions of the same variable
should have matching signatures, and the parameter and local variable lists for each function do not
have duplicate entries.


\paragraph{Semantics}

\jeehoon{defer memory model to the next subsection.}

\input{rtl-semdom}

We move on to the semantics of RTL.  Figure~\ref{fig:rtl-semdom} defines the necessary semantic
domains.

Memory, $m\in\rtlMem$, is represented as a finite collection of allocation blocks (a mapping from
block identifiers to blocks), each of which is a contiguous portion of memory that may be either
valid to access or already deallocated (freed) and therefore invalid to access.  CompCert values,
$v\in\rtlVal$, can be either 32-bit integers, logical addresses (pairs of a block identifier
together with an offset within the block), or the special $\rtlundef$ value used to represent
uninitialized data.  A global environment, $ge=(g,d)\in\rtlGEnv$, maps each global variable name to
a logical block identifier, and each logical block identifier corresponding to some function's code
to either the corresponding function signature for external functions or the corresponding function
definition for functions defined in the program.

Next, program states can be of three kinds: normal instruction states ($\rtlist$), call states
($\rtlcst$) just before passing control to an invoked function, and return states ($\rtlrst$), just
after returning from an invoked function.  Instruction states store the memory ($m$), the sequence
of parent stack frames ($s$), the definition of the function whose body is currently executed
($\vfd$), the current stack pointer ($sp$), the program counter ($pc$), and the contents of the
local registers ($rs$).  Call states record the memory, the stack, and the function to be called
($\vfds$) with its arguments ($args$).  The function to be called can be either an internal
function, in which case we record its definition, or an external one, in which case we record its
signature.  Return states record just the memory, the stack, and the value that was returned by the
function.  A stack $s$ is a list of stack frames, each of which records the same information as
normal instruction states, except with the addition of a register name $r$ where its return value
should be stored, and minus the memory ($m$) and stack ($s$) components.

The meaning of programs is described by three definitions:
\[
\begin{array}{r@{~~}c@{~~}l}
\rtlgenv &\in& \rtlProg \to \rtlGEnv \\
\rtlload &\in& \rtlProg \pfun \rtlState\\
\estep{} &\in& \powset{\rtlGEnv\times\rtlState\times\rtlEvent\times\rtlState}\\
\end{array}
\]
The first function, $\rtlgenv(\vprg)$, returns the global environment corresponding to the program:
it `allocates' the global variables of the program sequentially in blocks 1, 2, 3, and so on, and
maps the blocks corresponding to function symbols to the relevant function definition or signature.

Similarly, $\rtlload(\vprg)$ returns the initial state obtained by loading a program into memory: it
initializes the memory $m$ with the initial values of the global variables at the appropriate
addresses generated by $\rtlgenv(\vprg)$, and returns a call state, $\rtlcst~m~[\,]~\vfd~[\,]$,
where $\vfd$ is the function definition corresponding to \texttt{main()}.  Loading is a partial
function because it is undefined for programs without a \texttt{main()} function.

% \input{rtl-opsem}
% TODO

The $\estep{}$ relation is a small-step reduction relation describing how program states evolve
during the computation.  For clarity, we write $s \estep{\vevt}_{ge} s'$ instead of
$(ge,s,\vevt,s')\in {\estep{}}$.  The operational semantics for RTL is fairly standard and shown in
Figure~\ref{fig:rtl-opsem}: there is a rule for each of the various basic instructions of the
language.  Starting from normal instruction states, the instruction at the node pointed to by the
program counter is scrutinized ($\vfd@pc$).  Depending on what instruction is there, only one rule
is applicable.  The corresponding rule calculates the new values of the registers, the memory (for
store instructions), and the next program counter.  Calls and returns are treated a bit differently:
they do not directly transition from an instruction state to the next instruction state---they go
through an intermediate call/return state.

In more detail, calling a function (rule \textsc{call}) looks up the function in the global
environment, evaluates its arguments, creates a new stack frame corresponding to the current
instruction state, and transitions to a call state.  From a call state, there are two possible
execution steps.  If the function to be called is internal, \ie we have its function definition
$\vfd\in\rtlFunc$, rule \textsc{internal-call} applies.  It allocates the necessary stack space for
the called function, initializes the parameter registers with the values passed as arguments, sets
the program counter to point to the first node of the called function, and moves to the appropriate
instruction state of the called function.  If the function to be called is external, \ie we have a
function signature $\vfs\in\rtlFSig$, rule \textsc{external-call} goes directly to the return state,
and generates an event $\vevt$ indicating that it called an external function.

Conversely, returning from a function (rule \textsc{return}) evaluates the result to be returned,
deallocates the stack space used by the function, and transitions to the return state.  The only
possible step form a return state (rule \textsc{return2}) then pops the top-most stack frame and
transitions to a normal instruction state thereby restoring the registers, program counter, and
stack pointers of the calling function.



\subsection{The Memory Model}
\label{sec:background:memory}

To simplify the presentation, we focus on handling integer-pointer casts and relaxed-memory
concurrency and do not discuss many of the orthogonal aspects of C semantics.  Specifically, we
$(1)$ assume a 32-bit architecture: words are 4 bytes wide, and the size of the address space is
$2^{32}$, $(2)$ consider only integer and pointer values, and omit values of other types such as
\texttt{float} or \texttt{char}, and $(3)$ omit subword arithmetic, and assume each address stores a
32-bit value.  \jeehoon{check it.}
% and take all addressing to be aligned at 4-byte boundaries.  \todo{gil: this is not quite
% right. For example, we have a 4-byte word at 0x0000FFF0 and another 4-byte word at 0x0000FFF1,
% ratherthan at 0x0000FFF4.  We used this just for simplicity anyway.}  $(l,i)$ points to the $i$-th
% cell of 4 byte data in the logical block $l$).


\paragraph{Concrete Model}

Concrete memory consists of a $2^{32}$-sized array of values, and a
list of allocated blocks, 
represented as pairs $(p,n)$ of the block's starting address and its size.
Loading from or storing to an unallocated address raises
an error (\ie undefined behavior).  Values are just 32-bit integers,
since pointers are merely integers in the concrete model.
As a result, \emph{the concrete model natively supports integer-pointer casts}.
\[
\begin{array}{@{}l@{~}c@{~}l@{}}
\mathrm{Mem} &\defeq& (\inttype \to \mathrm{Val}) \times \mathtt{list}\ \mathrm{Alloc} \\
\mathrm{Alloc} &\defeq& \setof{(p,n) \suchthat p \in \inttype \land n \in \inttype} \\
\mathrm{Val} &\defeq& \setof{i\in\inttype}
\end{array}
\]

Memory allocation inserts a block into the list of allocated blocks, 
whereas deallocation removes one.  Overall, the list of allocated 
blocks should be \emph{consistent}:\footnote{These are a subset of 
  \texttt{malloc}'s properties according to the C11 Standard~\cite{iso2011iec}. 
  For more details, see \S7.22.3 paragraph~1 and \S6.5.8 paragraph~5.}
\begin{itemize}
\item If $(p, n)$ is allocated, then $\emptyset \neq [p,p+n) \subseteq
  (0,2^{32}-1)$.
%\item If $(p, n)$ is allocated, then $p\neq 0$ and $n\neq 0$.
\item If blocks $(p_1, n_1)$ and $(p_2, n_2)$ are distinct
  allocations, their ranges $[p_1,p_1+n_1)$ and $[p_2,p_2+n_2)$
  are disjoint.
\end{itemize}

As we have seen, however, the concrete model \emph{does not support 
standard compiler optimizations} such as constant propagation and 
dead allocation elimination in the presence of external function
calls.  This is because the model does not provide 
a mechanism for ensuring that a module has exclusive control over some
part of memory, thereby assuming that unknown code can read and 
update the contents of every allocated memory cell.


\paragraph{CompCert's Logical Model}

In CompCert's logical model
\cite{leroy:compcert,Leroy-Appel-Blazy-Stewart-memory-v2}, memory
consists of a finite set of logical blocks with unique block
identifiers.  Each block is a fixed-sized array of values together
with a validity flag $v$ that indicates whether the block is accessible or
has been freed. As before, accessing a freed block raises an error.
Values are either 32-bit integers or logical addresses.  Here, a
logical address $(l,i)$ consists of a block identifier $l$ and an
offset $i$ inside the block.
% (For simplicity, throughout this paper, we assume pointers
%are aligned at 4-byte boundaries and $(l,i)$ represents the 4-byte chunk at
%offset $4i$. \todo{This is not right for the same reason as before.})
%For a block identifier $l$, we henceforth write the 
%block $l$ for the block with identifier $l$.
\[
\begin{array}{@{}l@{~}c@{~}l@{}}
\mathrm{Mem} &\defeq& \mathrm{BlockID} \fpfun \mathrm{Block} \\
\mathrm{Block} &\defeq&
\setof{(v, n, c) \suchthat
  v \in \mathtt{bool} \land n \in \Nat \land c \in \mathrm{Val}^n } \\
\mathrm{Val} &\defeq& \setof{i\in\mathtt{int32}} \uplus \setof{(l,i) \in \mathrm{BlockID}\times \mathtt{int32}}
\end{array}
\]

An important advantage of the logical model over the concrete one is that 
it \emph{allows functions to have exclusive control over a logical block} as 
long as they do not allow its address to escape. The reason is that it
is not possible to manufacture the logical address of an already allocated block.  
This property guarantees the correctness of many useful optimizations, 
such as constant propagation across function calls and dead allocation
elimination.

A secondary advantage is that programs have \emph{infinite memory}, rendering
their allocation behavior unobservable, which in turn makes it easy for
compilers to remove dead allocations.

%% there is no out-of-meory behavior since the memory has infinitely many logical blocks.
%This is important because even if a source program consumes a huge
%amount of memory, ... (\todo{Viktor, please say something about this
% :)})

Apart from that logical models have a slightly more complicated
semantics, their main disadvantage is that \emph{they do not support
  integer-to-pointer casts} very well.
%% The main disadvantages of logical models are that they have a slightly
%% more complicated semantics and, most importantly, that \emph{they do
%%   not support integer-to-pointer casts} very well.  
As a result,
CompCert has very weak support for integer-pointer casts.  Generally,
they are treated as \texttt{nop}s (\ie the identity function) rather
than undefined (\ie erroneous), and thus variables of integer (or
pointer) types can contain both integers and logical addresses.  In
CompCert's higher-level languages (CompCert C and Clight), once a
pointer is cast into an integer, any arithmetic on that integer
returns an unspecified value.  In its lower-level languages (Cminor,
RTL, \emph{etc.}), however, some of the integer operations (namely,
addition, subtraction, and equality tests) are also well-defined for
pointer values in special cases.  More specifically, for instance,
%% subtraction and equality test are well defined for pointer values in
%special cases: \eg 
the addition of an integer to an address, the
subtraction between addresses when they are in the same logical block,
and the equality test between an address and \texttt{NULL} are well-defined.



\subsection{Constant Propagation}
\label{sec:background:constprop}

\paragraph{Algorithm}

Given a program $\vprg$, constant propagation walks through each function definition $\vfd$ of the program 
and transforms it using the function $\mathrm{transfun}(\vprg, \vfd)$.
This in turn runs a ``value analysis'' to determine which variables (whether global variables or local registers) hold a known constant value at each program point
and then, based on that information, simplifies the program.

The analysis consists of two parts: 
(a) the global part, which detects which global variables cannot be updated (\ie those declared with the \texttt{const} qualifier), and
(b) the local part, which analyzes the code of a function and calculates an abstract value for each register and stack variable.
The abstract value of a variable can be either $\bot$ if the variable holds $\tt undef$, 
or a constant number, 
or $\NSmac$ if the variable contains anything except for a pointer pointing into the current stack frame, 
or $\top$ if no more precise information is known.
These abstract values form a lattice by taking the order $\bot \sqsubseteq \mathit{num} \sqsubseteq \NSmac \sqsubseteq \top$.


The value analysis performs a usual traversal of the code. 
When calling a function, 
if it can be determined that no memory address can point to the current stack frame 
and none of the function's arguments point to the current stack frame (\ie their abstract value is at most $\NSmac$), 
then the abstract value of the function's result is also $\NSmac$, and the abstraction of the stack memory is preserved.
If, however, a pointer to the current stack frame has escaped, then any information about the stack memory is forgotten.

The transformation part itself is straightforward: 
at each node $n$ of the function's CFG, if the analysis has determined that a variable has a constant value at node $n$,
then the use of that variable is replaced by the constant it holds, and the instruction is suitably simplified.

As an example, 
Figure~\ref{fig:const-prop-example} shows the effect of constant propagation applied to a simple program.
The program contains one internal function, \texttt{f}, which calls an external function, \texttt{g},
and three zero-initialized variables: 
a local variable (a register), \texttt{x}, 
an address-taken variable on the stack, \texttt{sp[0]},
and a global variable, \texttt{gv[0]}.
After the external function call, 
constant propagation can safely assume that $\texttt{x}=0$ and thereby simplify the conditional at node 5,
but cannot do the same for \texttt{sp[0]} at node 4
because its address was passed to the external function and its value might therefore have changed.
Further, at node 6, 
constant propagation notes that the global variable \texttt{gv[0]} has been declared with the \texttt{const} qualifier,
and can therefore assume that $\texttt{gv[0]}=0$.


% \input{constprop-example}
% TODO

\jeehoon{explain the examples in the intro.}



\paragraph{Verification}

The correctness proof of the constant propagation pass in CompCert establishes 
the existence of a simulation relation, $R$, that relates the loading of the source and target programs.
That is, for every well-formed source RTL program $\vprg$, it proves 
there exists a simulation $R(\vprg)$ such that 
$(\mathrm{load}(\vprg),\mathrm{load}(\mathcal{T}_\mathrm{cp}(\vprg)))\in R(\vprg)$.

The simulation relation, $R$, used for the constant propagation pass is given in
Figure~\ref{fig:part0-rel}.  It is defined in terms of matching relations on states, stack frames,
and stacks and function definitions ($\simState$, $\simFrame$, $\simStack$, and $\simFun$
respectively).  These relations take as a parameter the source program, $\vprg$, which is used to
relate the function definitions of the source and target programs.\footnote{The version shown here
  is a slight simplification of the actual simulation used in the constant propagation pass.  It
  abstracts away some tedious details of the actual $\simFrame$ definition that are orthogonal to
  our story.  This is merely to streamline the presentation.}

We say that two function definitions are related in the program $\vprg$, written $\vfd \simFun \vfd'$, 
if the target function, $\vfd'$, is the result of applying constant propagation to the source function, $\vfd$.
Two stack frames are related by $\vprg \vdash \vsf\simFrame \vsf'$ if 
the function code in $\vsf'$ is the transformation of the function code of $\vsf$,
the stack pointer and program counters agree, and
the registers of $\vsf'$ hold equal or more defined values than those of $\vsf$.
Two stacks are related, $\vprg \vdash s\simStack s'$, if they have the same length and their stack frames are related elementwise.

% \input{part0-simrel}
% TODO

Two states are related, $\vprg \vdash s \simState s'$
if $(1)$ they are of the same kind, 
$(2)$ the memory of $s'$ is an extension of that of $s$, 
%\todo{jeehoon: do we need to explain lessdef and extends relations?}
$(3)$ their stacks are related by $\simStack$,
$(4)$ the respective function definitions are related by $\simFun$ (when applicable),
$(5)$ the stack pointer and program counter agree (when applicable), and
$(6)$ the registers/arguments/return value of $s$ is equal or less defined than that of $s'$. 

Finally, the two states are in the simulation relation $R$
if they are related by $\simState$ and 
the source state satisfies the value analysis invariant, $\soundstate(\vprg,s)$.
This invariant basically says that the (concrete) value of each variable in the state $s$ is included in the variable's abstract value computed by the analysis at the current program point.
The invariant depends on the program for two reasons:
$(1)$ so that it can calculate the global environment, $ge=\rtlgenv(\vprg)$, and
$(2)$ so that it can `run' the analysis on the program so as to be able to compare its results with the current state.


The basic soundness properties of the value analysis are
$(1)$ that the $\soundstate$ invariant holds for the initial state of a program, and
$(2)$ that it is preserved by execution steps.
Formally:
\[
\frac{s = \rtlload(\vprg)}{
  \soundstate(\vprg,s)
}
\quad~~
\frac{ 
\begin{array}{c}
  \soundstate(\vprg,s) \\[-4pt]
  ge = \rtlgenv(\vprg)  \qquad
  s \estep{t}_{ge} s' 
\end{array}
}{
  \soundstate(\vprg,s')
}
\]

The CompCert proof then establishes $(1)$ that
\[(\mathrm{load}(\vprg),\mathrm{load}(\mathcal{T}_\mathrm{cp}(\vprg)))\in
R\] and $(2)$ that $R$ is a simulation relation. As for $(1)$, the
initial states after loading satisfy $\simState$ by construction, and
the initial source state satisfies $\soundstate$ thanks to the
soundness of the value analysis above.

It remains to show that $R$ is indeed a simulation.  Specifically,
CompCert shows that it is a ``forward'' simulation, meaning that for
any related states $(s,t)\in R$, if the source state $s$ takes a step
to $s'$ with an event $\sigma$, the target $t$ also takes a step to
\emph{some} state $t'$ with the same event $\sigma$, such that $(s',t')
\in R$.  From $(s,t)\in R$, we know that we are executing the
instructions at the same $\vpc$. Thus by the definition of constant
propagation, the target instruction is either identical to the source
instruction or obtained by replacing a variable with a constant or
converting a conditional jump to an unconditional jump, depending on
the value analysis result.  Here, thanks to the soundness of the
current state w.r.t.\ the analysis result and the relation between the
two states specified by $\simState$, we can easily deduce that
executing the source and target instructions results in related
states.  Also, the soundness of the new source state $s'$ follows
from the soundness preservation property of the value analysis stated
above.

Finally, we briefly discuss why CompCert establishes a forward
rather than a backward simulation, even though the former implies that
the source's behaviors refine the target's, which seems the wrong way
around.  First, a forward simulation is easier to establish than a
backward one because a single instruction in the source may be
compiled down to several instructions in the target. Second, CompCert
composes forward simulations of all passes using the transitivity of
forward simulations, which is not hard to show. Then it converts the
composed forward simulation between C and assembly to a backward
simulation between them using some technical properties of C and
assembly (namely, that C is \emph{receptive} and assembly is
\emph{determinate}). Finally, from this backward simulation, one can
establish that the target's behaviors refine the source's.

\jeehoon{explain backward simulation here, and defer discussion on forward simulation to
  \Cref{chap:sepcomp}.}


% Example language from intptrcast
%
% For concreteness, we consider the following simple C-like language:
% \[
% \begin{array}{@{}l@{~}c@{~}l@{}}
% \mathit{Typ} &::=& \texttt{int}~|~\texttt{ptr}\\
% \mathit{Bop} &::=& \texttt{+}~|~\texttt{-}~|~\texttt{*}~|~\texttt{\&\&}~|~\texttt{=}\\
% \mathit{Exp} &::=& \mathit{Int}~|~\mathit{Var}~|~\mathit{Global}~|~\mathit{Exp}\ \mathit{Bop}\ \mathit{Exp}\\
% \mathit{RExp} &::=& \mathit{Exp}~|~\texttt{malloc}(\mathit{Exp})~|~\texttt{free}(\mathit{Exp})~|~\texttt{(}\mathit{Typ}\texttt{)}\ \mathit{Exp}\\&&|~\texttt{input()}~|~\texttt{output}(\mathit{Exp})\\
% \mathit{Instr} &::=& \mathit{Fid}(\mathit{Exp}, \ldots, \mathit{Exp});
% ~|~\mathit{Var}\ \texttt{=}\ \mathit{RExp}
% ~|~\mathit{Var}\ \texttt{=}\ \texttt{*}\mathit{Exp}
% \\&&|~\texttt{*}\mathit{Exp}\ \texttt{=}\ \mathit{Exp}
% ~|~\texttt{if}\ \texttt{(}\mathit{Exp}\texttt{)}\ \overline{\mathit{Instr}}\ \texttt{else}\ \overline{\mathit{Instr}}
% \\&&|~\texttt{while}~\texttt{(}\mathit{Exp}\texttt{)}\ \overline{\mathit{Instr}}\\
% \mathit{Decl} &::=& \mathit{Fid}(\mathit{Typ}\ \mathit{Var}, \ldots, \mathit{Typ}\ \mathit{Var})\\&&\texttt{\{}\texttt{var}~\overline{\mathit{Typ}\ \mathrm{Var}};\,\overline{\mathit{Instr}}\texttt{\}}
% \end{array}
% \]
% The \texttt{input} and \texttt{output} operations produce externally visible events; all other operations are intended to model the corresponding operations in C.
% By $\overline{T}$ we mean a list of $T$.
% For simplicity, we omit \texttt{return} instructions and instead return values via pointer-valued arguments to functions.

%Our language has two types of values, integers and pointers, where the latter is represented as a pair $(l, i)$ of block identifier and integer offset. As a convention, we will use $\texttt{a}, \texttt{b}, \texttt{c}...$ for integer variables and $\texttt{p}, \texttt{q}, \texttt{r}...$ for pointer variables. 



\paragraph*{}

\jeehoon{Transition.}  It is important to note that this approach of using forward simulations (when
convenient) carries over without modification when porting CompCert to Level A and B compositional
correctness, as we do in the next section.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
