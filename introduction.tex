\section{Introduction}
\label{sec:introduction}

\subsection{The Context: The C Programming Language}
\label{sec:introduction:context}

\paragraph{Hardware Abstraction}

The C programming language is used as the \emph{lingua franca} for systems programming, mainly due
to its two notable advantages: \emph{portability} and \emph{control} over hardware.  C is portable
in the sense that C programs can be compiled and then executed in most of the existing hardware.  At
the same time, C has a precise control over hardware in the sense that C allows programmers to
access low-level hardware features such as memory layout and concurrency.  These advantages for
decades have attracted system programmers, who have built a giant ecosystem around the C language
itself and tools such as optimizing compilers, linkers, and program analyzers.

C enjoys portability and control---seemingly conflicting properties---at the same time thanks to the
fact that it is a balanced abstraction over various assembly languages, which are the interface of
hardware to software developers.  If C were exposing too much detail of hardware, then it would have
not been able to support some assembly languages that mismatch with the exposed details, losing a
significant degree of portability; on the other hand, if C were exposing too little detail of
hardware, then it would have lost the precise control over them.  The design choice of C as a
hardware abstraction is so popular that other systems programming languages---such as C++, D,
Objective C, Swift, and Rust---largely follow the design of C and are often called ``C-like''.

% To summarize, the C programming language is an abstraction that should satisfy the desiderata for
% three different ``masters'': portability for programmers, control for hardware, and optimization for
% compilers.

% C---like all the other programming languages---serves multiple ``masters'', namely programmers,
% compilers, and hardware.  From programmer's point of view, C should support \emph{reasoning
%   principles} that are powerful enough to reason about real-world C programs and guarantee their
% safety and functional correctness.  On the other hand, C should validate compiler and hardware
% \emph{optimizations} that may vastly accelerate the execution of C programs and are therefore
% actually performed in the real-world compilers and hardware.  What is particularly interesting about
% C is that ...


\paragraph{Compiler Optimization}

However, C is not just a thin wrapper around assembly languages because of compiler optimizations.
They have been so crucial for the performance of systems since the early days that every system
programmer expect a compiler to perform, \eg{} register promotion~\cite{mem2reg} and register
allocation~\cite{regalloc}, which are very effective and yet quite sophisticated compiler
optimizations.  Optimizations are becoming more and more important these days because recent
hardware trends---such as SIMD, GPU, and accelerators---offer potential for compiler optimizations
to further improve the performance of systems.  Since optimizations are an essential ingredient of
the real-world practice of C programming, the language should be an abstraction not only over just
hardware assembly languages but also over compiler optimizations.

% system programmers are building bigger systems, which they cannot hand-optimize on their own; and
% hardware vendors are introducing complex features, which need a special attention for maximal
% utilization.

% These days the mainstream compilers are becoming so aggressive in these days that they are
% performing even subtle optimizations that cannot be immediately justified.

Designing an abstraction over both hardware assembly languages and compiler optimizations, however,
is a challenging task.  For instance, consider the following transformation that is actually
performed by the mainstream compilers such as GCC~\cite{gcc} and LLVM~\cite{llvm}:

\[\begin{array}{rcl}
\begin{minipage}{0.27\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(a);
}
\end{minted}
\end{minipage}
&
\optarrow
&
\begin{minipage}{0.4\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(42); // const. prop.
}
\end{minted}
\end{minipage}
\end{array}\]

\noindent Suppose \code{g()} is an external function whose body is unknown to the compiler, and
\code{print(a)} prints the value of \code{a} to the screen.  The function \code{f()} first assigns
\code{42} to the local variable \code{a} (line \code{1}), calls some unknown external function
\code{g()} (line \code{2}), and then prints \code{a} (line \code{3}).  As an optimization,
mainstream compilers replace \code{a} with \code{42} at line \code{3}, effectively propagating the
constant \code{42} at line \code{1} to line \code{3}.  Such a \emph{constant propagation}
optimization is valid even in the presence of a function call to the unknown function \code{g()}, at
least in the viewpoint of compilers, because the address of the variable \code{a} is not leaked to
\code{g()} and thus \code{g()} cannot modify the content of \code{a}.

But what if \code{g()} is adversarially trying to \emph{guess} the address of \code{a} as follows?
%
\[
\begin{minipage}{0.8\textwidth}
\begin{minted}{c}
void g() {
1: int anchor;
2: int *guess = &anchor + 10; // guessing &x
3: *guess = 666;
}
\end{minted}
\end{minipage}
\]
%
\noindent The function \code{g()} tries to guess the address of \code{a} by exploiting the fact that
stack usually grows downwards with a fixed offset: it first declares a variable \code{anchor} and
guesses that \code{a} is located 10 words later than than \code{anchor} is.  Alas, the guess happens
to be correct when the adversarial \code{g()} is separately compiled with GCC 8.2.1 and then linked
with the original \code{f()}, and in this case, \code{f()} will surprisingly print the evil value
666.  On the other hand, when \code{g()} is linked with the optimized \code{f()}, \code{f()} will
print the expected value 42.\footnote{We got these results with GCC 8.2.1 and compile option
  \code{-fno-stack-protector} in Linux 4.18.} This example invalidates the compiler's analysis that
\code{a} is accessible only within the function \code{f()}, putting the soundness of the
optimization in danger.


\paragraph{Undefined Behavior}

In order to rescue the soundness of constant propagation, C blames the adversarial \code{g()} by
marking it as invoking \emph{undefined behavior}~\cite[\S3.4.3p1]{c18}: \code{g()} is not following
the rule of C so that compilers can do anything it chooses, \eg{} even ``to make demons fly out of
your nose''~\cite{nasal-demons}.  Specifically, \code{g()} invokes undefined behavior in the ISO C18
standard~\cite{c18} because the pointer \code{guess} is derived from the memory allocation of
\code{anchor} and yet is outside of the valid range of \code{anchor}, rendering itself invalid as an
address~\cite[\S6.5.6p8]{c18}.  In other words, compilers may safely assume that all the pointers
derived from \code{anchor} \emph{shall} point to \code{anchor}, and a failure to conform with such
an assumption gives compilers the right to generate arbitrary binary code.

In the course of rescuing the soundness of constant propagation, the ISO C18 standard is
intentionally deviating from the hardware assembly language, where pointers have the same
representation as integer values of the appropriate width, and they simply index into a single flat
array representing memory.\footnote{This ``concrete'', flat memory model is supported via virtual
  page mapping in the operating systems.}

% The constant propagation example above shows that the low-level control over memory layout via
% pointer manipulation conflicts with a simple compiler optimization, and C resolves the conflict by
% marking the accesses to the outside of a memory allocation's valid range as invoking undefined
% behavior.

This kind of conflicts and resolutions (using undefined behavior) are common in the design of the C
programming language semantics.  For example, the example above shows that low-level control over
memory layout via pointer manipulation in C conflicts with a simple compiler optimization, and the
conflict is resolved by marking the accesses to the outside of a memory allocation's valid range as
invoking undefined behavior.  More generally, the C language has many conflicts between control over
various low-level details of hardware and various compiler optimizations implemented in mainstream C
compilers, and these conflicts are usually resolved by marking some unintended usage of language
features as invoking undefined behavior.

It is worth comparing C with higher-level languages---such as Java, C\#, OCaml, Haskell---that do
not need undefined behaviors to justify compiler optimizations thanks to their lack of precise
control over low-level details of hardware, \eg{} constant propagation is immediately justified in
Haskell without resorting to undefined behavior thanks to its lack of raw pointer.

% Indeed, C has a lot of undefined behavior instances in the language semantics.  The C language is
% particularly interesting in that..  the widespread use of undefined behavior for resolving



\subsection{The Problem: Reckless Development of Semantics and Compilers}
\label{sec:introduction:problem}

\jeehoon{reorganize it}

% The ISO C18 standard~\cite{c18} famously does not give semantics to a significant subset of
% syntactically valid C programs. Instead, many programs exhibit implementation-defined, unspecified,
% or undefined behavior, with the latter imposing no requirements on a conforming implementation.

This has led to the somewhat controversial practice of sophisticated C compilers reasoning backwards
from instances of undefined behavior to conclude that, for example, certain code paths must be
dead. Such transformations can lead to surprising non-local changes in program behavior and
difficult-to-find bugs~\cite{wang2013towards,yang2011finding}.

Accordingly, there have been numerous efforts to capture the subtleties of the C standard formally,
either by giving an alternative language definition or a conforming
implementation~\cite{norrish1998c,leroy:compcert,ellison2012executable}.

The C memory model has been of particular interest: cross-platform low-level access to memory is a
defining feature of C-like languages and is essential for applications such as operating system
kernels and language runtimes.  However, subtle pointer aliasing rules~\cite{krebbers2013aliasing},
reliance on implementation-specific behavior, and the treatment of pointers to uninitialized memory
makes reasoning about even single-threaded programs non-trivial.

The problem is that the C language and its compilers have evolved in such an unplanned way that even
experienced system programmers disagree on the semantics of numerous language features and the
soundness of various compiler optimizations.  In order to improve performance and energy consumption
of systems, compiler writers have introduced dozens of subtle optimizations even though their
soundness is justified solely by intuition; in turn, to justify those optimizations afterwards, the
recent ISO C standards mark certain programs as invoking undefined behavior with a variety of ad-hoc
exceptions, making the already informal semantics written in English more ambiguous and confusing.
In order to mitigate the problem caused by reckless development of C semantics and compilers, ISO
have revised the C semantics in a series of standards---C89, C99, C11, and C18---but they are still
complex and are not widely accepted in the systems programming community, \eg{} the Linux community
defines its own dialect of C that supports much less compiler optimizations and is closer to
assembly languages than C18.


\paragraph{Too Many Undefined Behaviors}

\jeehoon{explain the problem with examples: too many undefined behaviors}


\paragraph{Conflicts among Optimizations}

Even worse, some optimizations in mainstream compilers are conflicting with each other and result in
miscompilation bugs.  It is difficult to fix these bugs because it is unclear which optimizations
are to blame.

For example, we discovered that GCC miscompiles the following program\footnote{This miscompilation
  bug is reported in \url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=65752}, which is still open
  as of this writing.}:
%
\begin{figure}
\begin{center}
\small
\begin{minipage}{0.5\textwidth}
\begin{minted}[bgcolor=white]{c}
void main() { 
  int x = 0;
  uintptr_t xi = (uintptr_t) &x;
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=gray]{c}
  uintptr_t i;
  for (i = 0; i < xi; ++i) {}
  if (xi != i) {
    printf("unreachable\n");
    xi = i;
  }
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=white]{c}
  auto p = (int *) xi;
  *p = 1;
  printf("%d\n", x); // expected: 1, actual: 0
}
\end{minted}
\end{minipage}
\end{center}
\caption{A GCC bug in the presence of integer-pointer casts}
\label{fig:introduction:bug}
\end{figure}

\noindent (At first, ignore the gray area.)  Note that \code{uintptr\_t} is an integer type that is
able to hold a pointer value~\cite[\S7.20.1.4]{c18}.  In this program, the pointer to the local
variable \code{x} is cast to an integer, \code{xi}, and then cast back to pointer, \code{p}.
Thus \code{p} points to \code{x}, and after \code{*p} is assigned one, the value of \code{x} should
be one.  Now inserting the code in the gray area should not change the program's behavior, because
the gray area is dead code: after the for loop, \code{i} equals to \code{xi}, and the conditional
branch is not taken.

GCC miscompiles this program as follows:
%
\begin{enumerate}
\item Code motion optimization moves \mintinline{c}{xi = i} out of the conditional branch, because
  regardless of whether the branch is taken, \code{xi = i} should hold.
\item Alias analysis thinks that \code{p} is not a valid pointer, because it originates from
  \code{xi}, which equals to \code{i}, which in turn is obtained by just incrementing by one several
  times from zero.
\item Constant propagation optimization replaces \code{x} at the last line with zero, because
  \code{x} is initialized with zero and no intervening stores are writing to \code{x}.  In
  particular, \code{p} is not aliased with \code{\&x}, at least from the compiler's point of view,
  because \code{p} is not a valid pointer.
\end{enumerate}

\noindent It is unclear which optimizations or analyses involved with this bug are to blame, because
all of them look fine: the code motion and constant propagation optimizations are definitely legit,
and the alias analysis has a reasonable cause to believe that \code{p} is invalid.  But still, they
are conflicting with each other and they collectively result in a miscompilation bug.


\subsection{The Prior Art: Formal Semantics and Compiler Verification}

In order to systematically address the problems caused by the unplanned evolution of C semantics and
compilers, researchers have proposed to \emph{define the formal semantics} of C and \emph{prove the
  soundness of compiler optimizations} w.r.t. the formal semantics.  In this research agenda, we
describe the C semantics no longer in an informal English prose (as the ISO C18 standard does) but
in a mathematically clear formalization, thereby completely removing the ambiguity in the semantics;
furthermore, based on the formalized semantics, we prove that compiler optimizations preserve the
semantics of source program, conclusively vindicating them from miscompilation bugs.

A landmark in this agenda is the CompCert C compiler~\cite{compcert}, which was initiated by Xavier
Leroy over ten years ago and grows as the first realistic verified compiler.  The CompCert compiler
is realistic in the sense that it ``could realistically be used in the context of production of
critical software''.  In particular, it compiles a significant subset of ISO C99 down to assembly,
and it performs a number of common and useful optimizations.  It is verified in the sense that it
``is accompanied by a machine-checked proof [in Coq] of a semantic preservation property: the
generated machine code behaves as prescribed by the semantics of the source program.''  As such,
CompCert guarantees that program analyses and verifications performed on its input carry over
soundly to its machine-level output.  It has served as a fundamental building block in academic work
on end-to-end verified software~\cite{TODO}, as well as receiving significant interest from the
avionics industry~\cite{TODO}. \jeehoon{say nuclear power plant}

% Over ten years ago, Xavier Leroy initiated the CompCert
% project~\cite{compcert:CACM,leroy:compcert}, a landmark effort that
% resulted in the first \emph{realistic verified compiler}.  The
% CompCert compiler~\cite{compcert-website} is \emph{realistic} in the
% sense that it ``could realistically be used in the context of
% production of critical software''.  In particular, it compiles a
% significant subset of the C language down to assembly, and it performs
% a number of common and useful optimizations.  It is \emph{verified} in
% the sense that it ``is accompanied by a machine-checked proof [in Coq]
% of a semantic preservation property: the generated machine code
% behaves as prescribed by the semantics of the source program.''  As
% such, CompCert guarantees that program analyses and verifications
% performed on its input carry over soundly to its machine-level output.
% It has served as a fundamental building block in academic work on
% end-to-end verified software~\cite{appel:plcc}, as well as receiving
% significant interest from the avionics industry~\cite{avionics}.
% % http://projects.laas.fr/IFSE/FMF/J3/slides/P05_Jean_Souyiris.pdf
% % XXX: Any other projects that depend on CompCert besides Appel's
% % XXX: Verified Software Toolchain or Airbus?

In the same spirit as CompCert, Vellvm~\cite{vellvm} by Steve Zdancewic and his collaborators
formalizes a significant subset of the LLVM IR (intermediate representation) and verifies
interesting compiler transformations and optimizations performed at the IR level.  Most notably,
Vellvm formalizes the static single assignment form (SSA), including a dominance analysis, an SSA
type checker, and an SSA-aware register promotion algorithm that is simplified from the
\code{mem2reg} pass in LLVM.

\jeehoon{Explain the benefit of formalization in more details.}

However, these projects make big simplifying assumptions on C semantics and compilers, skating over
the complexity of the real-world practice of systems programming.  While CompCert and Vellvm support
a significant subset of C99 and LLVM IR, respectively, they lack support for various low-level
features that are crucially used in many system programs, such as memory layout, concurrency, and
processor register manipulation.  Furthermore, CompCert and Vellvm only perform quite
straightforward transformations and optimizations that are way less sophisticated than those in
mainstream compilers, and support only limited use cases of compilers, \eg{} they did not verify
linking.  Because of these simplifications, they are currently suitable only for a niche market such
as safety-critical embedded systems.

\jeehoon{More acutely articulate the problem of the previous projects.}

\jeehoon{There's a more broad problem and contribution, I think.}



\subsection{Our Contribution: Towards Formalization of C in the Wild}

In this dissertation, we carry forward the research agenda for the formalization of real-world
practice of C semantics and compilers by accounting for more features and use cases that are crucial
in the real-world practice and yet were beyond the reach of the prior work.  Specifically, we make
the following contributions:


% Specifically, we propose the formal semantics of several low-level features that are the defining
% characteristics of C and yet are omitted from the prior work.  Furthermore, we propose
% verification techniques for real-world compiler use cases.

%
\paragraph{Cast between Integers and Pointers}

While cast between integers and pointers is one of the defining characteristics of the C programming
language, the feature has not been formalized in CompCert and Vellvm because it drastically
conflicts with major compiler optimizations.  The ISO C standards try to reconcile the feature and
the optimizations using the notion of \emph{provenance}, but it fails to support certain common
optimizations and requires an intrusive change to the language semantics.  In
\Cref{chap:intptrcast}, we propose the first formal semantics of casts between integers and pointers
that $(1)$ fully supports operations on the representation of pointers, including all arithmetic
operations for pointers that have been cast to integers, $(2)$ validates major compiler
optimizations on memory accesses, and $(3)$ is simple to understand and program with.

The key novel idea behind our semantics is the notion of \emph{concretization}: when allocated, a
memory block is not assigned a concrete address yet; only when it is required by a
pointer-to-integer cast, the block is lazily assigned a concrete address, \ie{} the block is
concretized.

In the course of doing this research, we discovered the GCC bug presented in
\Cref{fig:introduction:bug}, with which we persuaded compiler writers that it is safe to turn off
some too aggressive alias analyses.  Furthermore, our idea has subsequently been refined by
follow-up papers by other researchers~\cite{intptrcast-oopsla,intptrcast-popl}, which are
accompanied with promising revision proposals to the LLVM compiler and ISO C standard.


% The ISO C standard does not specify the semantics of many valid programs that use non-portable
% idioms such as integer-pointer casts. Recent efforts at formal definitions and verified
% implementation of the C language inherit this feature.  By adopting high-level abstract memory
% models, they validate common optimizations. On the other hand, this prevents reasoning about much
% low-level code relying on the behavior of common implementations, where formal verification has
% many applications.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{intptrcast} \textbf{Jeehoon Kang}, Chung-Kil Hur, William Mansky, Dmitri Garbuzov,
  Steve Zdancewic, Viktor Vafeiadis.  \emph{A Formal C Memory Model Supporting Integer-Pointer
    Casts}.  \textbf{PLDI 2015}.}

\paragraph{Relaxed-Memory Concurrency}

Despite many years of research, it has proven very difficult to develop a formal semantics for
concurrent programming languages that adequately balances the conflicting desiderata of programmers,
compilers, and hardware.  In \Cref{chap:relaxed}, we propose the first formal semantics of
relaxed-memory concurrency that $(1)$ accounts for a broad spectrum of low-level concurrency
features in C, $(2)$ is implementable, in the sense that it provably validates many standard
compiler optimizations and reorderings, as well as standard compilation schemes to x86-TSO,
$(3)$ justifies simple invariant-based reasoning, thus demonstrating the absence of bad
``out-of-thin-air'' behaviors, $(4)$ supports ``DRF'' guarantees, ensuring that programmers who use
sufficient synchronization need not understand the full complexities of relaxed-memory semantics,
and $(5)$ defines the semantics of racy programs without relying on undefined behaviors, which is a
prerequisite for applicability to type-safe languages like Java.

The key novel idea behind our semantics is the notion of promises: a thread may promise to execute a
write in the future, thus enabling other threads to read from that write out of order.  Crucially,
to prevent out-of-thin-air behaviors, a promise step requires a thread-local certification that it
will be possible to execute the promised write even in the absence of the promise.

Our semantics is a promising proposal for relaxed-memory concurrency semantics for C/C++ and
compiler IRs, and draws interest from both industry and academia.  In particular, our semantics
serves as a guide to C/C++ relaxed-memory concurrency, and reasoning principles for our semantics
are being developed.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{promising} \textbf{Jeehoon Kang}, Chung-Kil Hur, Ori Lahav, Viktor
  Vafeiadis, Derek Dreyer.  \emph{A Promising Semantics for Relaxed-Memory Concurrency}.
  \textbf{POPL 2017}.}


\paragraph{Lightweight Verification of Separate Compilation}

Major compiler verification efforts, such as CompCert and Vellvm, have traditionally simplified the
verification problem by restricting attention to the correctness of whole-program compilation,
leaving open the question of how to verify the correctness of separate compilation.  Recently, a
number of sophisticated techniques have been proposed for proving more flexible, compositional
notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the
simple ``closed simulations'' used in verifying whole-program compilation.  Applying such techniques
to a compiler like CompCert, as Stewart \etal{} have done, involves major changes and extensions to
its original verification.

In \Cref{chap:sepcomp}, we show that if we aim somewhat lower---to prove correctness of separate
compilation, but only for a single compiler---we can drastically simplify the proof effort.  Toward
this end, we develop several lightweight techniques that recast the compositional verification
problem in terms of whole-program compilation, thereby enabling us to largely reuse the
closed-simulation proofs from existing compiler verifications.  We demonstrate the effectiveness of
these techniques by applying them to CompCert 2.4, converting its verification of whole-program
compilation into a verification of separate compilation in less than two person-months.  This
conversion only required a small number of changes to the original proofs.

We uncovered two compiler bugs along the way---one of which is on separate compilation and the other
is orthogonal to separate compilation---and our verification techniques are subsequently adopted in
CompCert 2.7.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{sepcomp} \textbf{Jeehoon Kang}, Yoonseung Kim, Chung-Kil Hur, Derek Dreyer,
  Viktor Vafeiadis.  \emph{Lightweight Verification of Separate Compilation}.  \textbf{POPL 2016}.}

\paragraph*{}

To establish confidence in all our contributions, we have formalized most of our key results in Coq.
The formalization is available online~\cite{kang-phd-thesis-web}.

In summary, we propose the formal semantics of integer-pointer casts and relaxed-memory concurrency,
which are the defining characteristics of C and yet are omitted from the prior work because of their
inherent complexity.  Based on our work, revisions to the ISO C standard and mainstream compilers
such as GCC and LLVM are being prepared.  Furthermore, we generalize CompCert to support for
separate compilation use cases---the majority of the real-world use cases for embedded
software---with a low verification cost.  As a result, we close the gap between the theory and
practice of systems programming in several dimensions.  As future work, by further pursuing the
research agenda for the formalization of real-world practice of C semantics and compilers, we aim to
provide clear, theoretically-informed, and practically-relevant C semantics and compilers that
system programmers can immediately benefit from.  If successful, our formal semantics will be able
to replace the C dialects used in Linux and other systems.

% by formalizing the semantics of other complex low-level features---such as type-based alias
% analysis, union, inline assembly, GPUs and accelerators---and by verifying mainstream compilers.


% Other papers:
%
% \begin{itemize}
% \item[\cite{scfix}] Ori Lahav, Viktor Vafeiadis, \textbf{Jeehoon Kang}, Chung-Kil Hur, Derek Dreyer.
%   \emph{Repairing Sequential Consistency in C/C++11}.  \textbf{PLDI 2017}.
% \item[\cite{crellvm}] \textbf{Jeehoon Kang}*, Yoonseung Kim*, Youngju Song*, Juneyoung Lee, Sanghoon
%   Park, Mark Dongyeon Shin, Yonghyun Kim, Sungkeun Cho, Joonwon Choi, Chung-Kil Hur, Kwangkeun Yi.
%   (*The first three authors contributed equally and are listed alphabetically.)  \emph{Crellvm:
%     Verified Credible Compilation for LLVM}.  \textbf{PLDI 2018}.
% \end{itemize}


\paragraph{Organization}

\Cref{sec:background} provides the technical background on CompCert that informs the rest of this
dissertation.  \Cref{chap:intptrcast} presents a formal semantics of integer-pointer casts,
\Cref{chap:relaxed} presents a formal semantics of relaxed-memory concurrency, and
\Cref{chap:sepcomp} presents lightweight verification techniques for separate compilation.  This
dissertation concludes with \Cref{chap:epilogue}, which summarizes the main contributions and the
impacts and presents future research directions (\Cref{sec:conclusion}).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
