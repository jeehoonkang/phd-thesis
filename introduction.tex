\section{Introduction}
\label{sec:introduction}

The C programming language is used as the \emph{lingua franca} for systems programming, mainly due
to its three notable advantages: \emph{low-level features}, \emph{portability}, and
\emph{performance}.  C provides low-level features that offer programmers precise control over
hardware such as pointer manipulation, shared-memory concurrency, and asynchronous interrupt.  At
the same time, C is portable in that C programs can be compiled and then executed in most of the
existing hardware.  Furthermore, C provides decent performance: a program written in C are usually
outperforming equivalent programs written in other languages---even including carefully hand-written
assembly programs---when they perform exactly the same task.  These advantages for decades have
attracted system programmers, and as a result, a giant ecosystem was built around the C programming
language.

C provides low-level features and portability---seemingly conflicting properties---at the same time
because it is a balanced abstraction over various hardware assembly languages.  If C were exposing
too much detail of particular hardware architectures, then it would have not been able to support
mismatching architectures, losing a significant degree of portability; on the other hand, if C were
exposing too little detail of hardware architectures, then it would have not been able to provide
low-level features.  The design choice of C as a hardware abstraction is so popular that other
systems programming languages---such as C++, D, Objective C, Swift, and Rust---largely follow the
design of C and are often called ``C-like''.

% features, losing the opportunity to improve the performance.

% To summarize, the C programming language is an abstraction that should satisfy the desiderata for
% three different ``masters'': portability for programmers, control for hardware, and optimization for
% compilers.

% C---like all the other programming languages---serves multiple ``masters'', namely programmers,
% compilers, and hardware.  From programmer's point of view, C should support \emph{reasoning
%   principles} that are powerful enough to reason about real-world C programs and guarantee their
% safety and functional correctness.  On the other hand, C should validate compiler and hardware
% \emph{optimizations} that may vastly accelerate the execution of C programs and are therefore
% actually performed in the real-world compilers and hardware.  What is particularly interesting about
% C is that ...


\paragraph{Compiler Optimizations}

Being a simple syntax translator from C to assembly languages, however, is insufficient for C
compilers to provide the desired level of performance.  Instead, compilers perform
\emph{optimizations} that transform the given program to be executed more efficiently in hardware
while preserving its semantics.  Optimizations are so effective that they have been an essential
ingredient of compilers since the early days.  For example, every system programmer expects a
compiler to perform quite sophisticated optimizations such as register promotion~\cite{mem2reg} and
register allocation~\cite{regalloc}.  Optimizations are becoming more important these days because
recent hardware trends---such as SIMD, GPU, and accelerators---offer potential for compiler
optimizations to further improve the performance of systems.

The following is an instance of the \emph{constant propagation} optimization, which significantly
improves the performance of compilation results and is thus performed by mainstream compilers such
as GCC~\cite{gcc} and LLVM~\cite{llvm}:

%
\[\begin{array}{rcl}
\begin{minipage}{0.27\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(a);
}
\end{minted}
\end{minipage}
&
\optarrow
&
\begin{minipage}{0.4\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(42); // const. prop.
}
\end{minted}
\end{minipage}
\end{array}\]

\noindent Suppose \code{g()} is an external function whose body is unknown to the compiler, and
\code{print(a)} prints the value of \code{a} to the screen.  The function \code{f()} first assigns
\code{42} to the local variable \code{a} (line \code{1}), calls some unknown external function
\code{g()} (line \code{2}), and then prints \code{a} (line \code{3}).  As an optimization,
mainstream compilers replace \code{a} with \code{42} at line \code{3}, effectively propagating the
constant \code{42} at line \code{1} to line \code{3}.  Compilers perform such an optimization even
in the presence of a function call to the unknown function \code{g()}, because---at least in the
viewpoint of compilers---the address of the variable \code{a} is not leaked to \code{g()} and thus
its content cannot be modified by \code{g()}.

This optimization, however, may change the program's behavior, putting the soundness of the
optimization in danger.  For instance, suppose that \code{f()} is linked with the following
\code{g()}:
%
\[
\begin{minipage}{0.8\textwidth}
\begin{minted}{c}
void g() {
1: int anchor;
2: int *guess = &anchor + 10; // guessing &a
3: *guess = 666;
}
\end{minted}
\end{minipage}
\]
%
\noindent Here, the function \code{g()} tries to \emph{guess} the address of \code{a} by exploiting
the fact that stack usually grows downwards with a fixed offset: it first declares a variable
\code{anchor} and guesses that \code{a} is located 10 words later than than \code{anchor} is.  While
extremely dangerous and thus discouraged, the guess sometimes happens to be correct, invalidating
the compiler's reasoning that \code{a} is accessible only within the function \code{f()}.  If it is
the case, when linked with \code{g()}, the original \code{f()} will print the evil value 666; on the
other hand, the optimized \code{f()} will print the expected value 42.\footnote{We got these results
  by separately compiling \code{f()} and \code{g()} and then linking them using GCC 8.2.1 with
  compile option \code{-fno-stack-protector} in an x86-64 machine running Linux 4.20.}

To rescue the soundness of constant propagation, the ISO C18 standard~\cite{c18} blames \code{g()}
for violating the rule of the C programming language by marking it as invoking \emph{undefined
  behavior}~\cite[\S3.4.3p1]{c18}.  Specifically, line \code{2} invokes undefined behavior because
\code{guess} is derived from \code{anchor} and yet it does not point to the valid location within
\code{anchor}'s allocation~\cite[\S6.5.6p8]{c18}.  (Roughly speaking, all the pointers derived from
\code{anchor} \emph{shall} point to \code{anchor}; otherwise, the behavior is undefined.)  Now an
instance of undefined behavior allows compilers to do anything it chooses, from arbitrarily changing
the code and thereby justifying the constant propagation optimization to even making ``demons fly
out of your nose''~\cite{nasal-demons}.\footnote{Notice that undefined behavior is not necessary for
  higher-level languages---such as Java, C\#, OCaml, Haskell---to justify compiler optimizations
  thanks to their lack of low-level features.  For example, constant propagation is immediately
  justified in Haskell without resorting to undefined behavior thanks to its lack of raw pointer.}

Notice that C intentionally loses the ability to manipulate pointers in an unrestricted way---\eg{}
deriving the address of \code{a} from that of \code{anchor}---in order to justify the constant
propagation optimization.  This is beneficial because the performance improvement offered by
compiler optimizations outweights the cost of the restriction on the low-level feature for
unidiomatic programs.  As a result, pointers in C should have a richer structure than those in
assembly languages that have the same representation as integer values of the appropriate width and
simply index into a single flat array representing memory.

Such a practice of taming low-level features for supporting compiler optimizations is actually quite
common in the design of the C programming language: mainstream compilers perform aggressive
optimizations that may change the behaviors of programs that use low-level features in unidiomatic
ways.  As a result, C programs may have different meaning than the exactly same programs written in
assembly languages (modulo syntactic differences).  In other words, C is no longer just a thin
wrapper around assembly languages but it should rather be an abstraction over both assembly
languages \emph{and} compiler optimizations.

% C is effectively trading precise control over hardware offered by low-level features (especially for
% poorly written programs) for performance benefits offered by compiler optimizations.


\subsection{Conflict between Low-Level Features and Compiler Optimizations}
\label{sec:introduction:problem}

Unfortunately, despite many years of research and industrial efforts~\cite{c18}, it has proven very
difficult to adequately balance the conflicting criteria for low-level features and compiler
optimizations in the design of the C programming language.  On the one hand, C should support the
common usage patterns of the low-level features in systems programming, such as relaxed-memory
concurrency, separate compilation, and cast between integers and pointers.  In addition, programmers
should be able to reason about programs that use the low-level features.  On the other hand, C
should also support the sophisticated and yet effective optimizations performed by mainstream
compilers, such as register promotion, constant propagation, and dead code elimination.  To the best
of our knowledge, none of the existing proposals for C semantics sufficiently support both low-level
features and compiler optimizations at the same time.

% occasionally defeating the intention of programmers to precisely control over hardware.

% here the C programming language is effectively trading control for performance:

% , which significantly improves the performance of compilation results.

% The compiler's reasoning, however, seems wrong in certain circumstances, 

% Therefore C have richer structures than those in assembly languages due to compiler optimizations.

% The above transformation clearly shows that C is a very different language from assembly languages:
% the constant propagation optimization should sound for C programs---as it is performed by mainstream
% compilers---but it is unsound for assembly programs.

% Now suppose that \code{f()} in the above example were assembly programs (modulo syntactic
% differences).  Then the transformation would be incorrect in certain circumstances.

% threaten the portability of C programs by changing the source program's behavior in certain
% circumstances.

% suppose that \code{g()} is adversarially trying to \emph{guess} the address of \code{a} as follows:

% Since optimizations are an essential ingredient of the real-world practice of C programming, the
% language should be

% system programmers are building bigger systems, which they cannot hand-optimize on their own; and
% hardware vendors are introducing complex features, which need a special attention for maximal
% utilization.

% The constant propagation example above shows that the low-level control over memory layout via
% pointer manipulation conflicts with a simple compiler optimization, and C resolves the conflict by
% marking the accesses to the outside of a memory allocation's valid range as invoking undefined
% behavior.


\paragraph{Prior Work}

The ISO C standard, even after a series of revisions including C89, C99, C11, and C18, still has
quite unclear specification for some of the low-level features as of this writing.  First, ISO
C18~\cite{c18} informally describes the C programming language in English prose, which is often
ambiguous and confusing.  The problem has only worsened by the fact that the description contains
many ad-hoc exceptions including 203 cases of undefined behavior~\cite[J.2]{c18}.  Second, ISO C18
intentionally leaves the precise meaning of some of the low-level features undefined.  For example,
the semantics of cast between integers and pointers is not properly defined in ISO C18, while it is
essential for applications such as operating system kernels and language runtimes.

Accordingly, there have been numerous efforts to capture the subtleties of the ISO C standard by
giving an alternative formal language
definition~\cite{leroy:compcert,vellvm:popl12,ellison2012executable,norrish1998c,krebbers}.  However, all
these projects---while supporting a significant subset of ISO C99 or LLVM IR---make unrealistic
simplifying assumptions on C semantics and lack support for various low-level features.
% skating over the complexity of the real-world practice of systems programming.


As a result, many systems programming communities, \eg{} the Linux kernel developers, use their own
\emph{dialect} of C that is closer to assembly languages and supports more low-level features and
less compiler optimizations than the standard.  The dialects, however, are often informally
described as the set of turned-on compiler optimizations (\eg{} ``\code{gcc -O2}''), whose meaning
is unclear and unstable.


\paragraph{Problem}

% As a consequence, the meaning of C programs that use use low-level features remains unclear until
% today.

The unresolved conflict between low-level features and compiler optimizations causes difficulties to
both programmers and compiler writers.  For programmers, it is difficult to expect how programs that
use low-level features will behave because compilers may perform conflicting optimizations, which
introduce surprising non-local changes and difficult-to-find bugs in program
behavior~\cite{wang2013towards,yang2011finding}.  As a result, mainstream compilers are typically
unused for safety-critical systems or used with only few compiler optimizations turned on,
significantly increasing verification cost and degrading performance of safety-critical systems.
One the other hand, for compiler writers, it is difficult to figure out whether an optimization is
sound or not in the presence of low-level features.  Even worse, sometimes a combination of
optimizations---while each and every one of them seems legit---results in miscompilation bugs, for
which it is unclear how to fix.

% in particular, those bugs are not effectively found and fix by testing due to unclear specification.

% For instance, compilers reason backwards from instances of undefined behavior to conclude that,
% \eg{} certain code paths must be dead and to remove the code.  for example, certain code paths
% must be dead and to remove those paths.  to conclude that,

% To improve performance and energy consumption of systems, compiler writers have introduced dozens
% of subtle optimizations even though their soundness is justified solely by intuition.

% This kind of conflicts and resolutions (using undefined behavior) are common in the design of the
% C programming language semantics.  For example, the example above shows that low-level control
% over memory layout via pointer manipulation in C conflicts with a simple compiler optimization,
% and the conflict is resolved by marking the accesses to the outside of a memory allocation's valid
% range as invoking undefined behavior.  More generally, the C language has many conflicts between
% control over various low-level details of hardware and various compiler optimizations implemented
% in mainstream C compilers, and these conflicts are usually resolved by marking some unintended
% usage of language features as invoking undefined behavior.

% The ISO C18 standard~\cite{c18} famously does not give semantics to a significant subset of
% syntactically valid C programs. Instead, many programs exhibit implementation-defined, unspecified,
% or undefined behavior, with the latter imposing no requirements on a conforming implementation.


\paragraph{A Miscompilation Bug}

\begin{figure}[!]
\begin{center}
\small
\begin{minipage}{0.8\textwidth}
\begin{minted}[bgcolor=white]{c}
// c.c
#include <stdio.h>
#include <stdint.h>

void f(int*, int*);

int main()
{
 1: int a=0, y[1], x = 0;
 2: uintptr_t pi = (uintptr_t) &x;
 3: uintptr_t yi = (uintptr_t) (y+1);
 4: uintptr_t n = pi != yi;

 5: if (n) {
 6:   a = 100;
 7:   pi = yi;
 8: }
 9: if (n) {
10:   a = 100;
11:   pi = (uintptr_t) y;
12: }

13: *(int *)pi = 15;

14: printf("a=%d x=%d\n", a, x); // observed: a=0 x=0
15: f(&x,y);
16: return 0;
}

// b.c
void f(int*x, int*y) {}
\end{minted}
\end{minipage}
\end{center}
\caption{An LLVM bug in the presence of integer-pointer casts}
\label{fig:introduction:bug}
\end{figure}

\Cref{fig:introduction:bug} presents an LLVM miscompilation bug\footnote{This bug is reported in the
  LLVM bug tracker: \url{https://bugs.llvm.org/show_bug.cgi?id=34548}} due to conflicting
optimizations.  Note that the type \code{uintptr\_t} is an integer type that is capable of holding a
pointer value~\cite[\S7.20.1.4]{c18}.  For this program, the expected outcome is either \code{a=0
  x=15} or \code{a=100 x=0} for the following reasons:
%
\begin{itemize}
\item Suppose \code{n=0}.  Then \code{pi} points to \code{x} after line \code{12}.  Thus line
  \code{13} writes \code{15} to \code{x}, and the end result is \code{a=0 x=15}.

\item Suppose otherwise.  Then \code{a=100} and \code{pi} points to \code{y} after line \code{12}.
  Thus line \code{13} writes \code{15} to \code{y}, and the end result is \code{a=100 x=0}.
\end{itemize}
%
However, the result \code{a=0 x=0} is observed when \code{c.c} and \code{b.c} are compiled with
\code{clang -O2} and then executed due to the following series of optimizations:
\begin{enumerate}
\item The integer comparison \code{pi != yi} at line \code{4} is replaced with the pointer
  comparison \code{\&x != y+1}.
\item The compiler assumes \code{n=0}, which is allowed since now line \code{4} is comparing
  pointers from different origins.
\item Lines \code{5-12} is eliminated since the condition \code{n} evaluates to false.
\item \code{(int*)pi} at line \code{13} is replaced with \code{(int*)yi} since \code{n=0}, and then
  with \code{y+1} since \code{yi=(uintptr\_t)(y+1)}.
\item Line \code{13} is eliminated since it is writing to an invalid address \code{y+1}.  Then line
  \code{14} prints \code{a=0 x=0}.
\end{enumerate}
%
\noindent In short, LLVM has a miscompilation bug due to the conflict among the above five
optimizations.  However, each of them looks legit---at least in the viewpoint of the LLVM
compiler---and it is unclear which one(s) is to blame.  That is one of the reasons this bug is still
open in the bug tracker as of this writing.


\subsection{Reconciling Low-Level Features with Compiler Optimizations}

In this dissertation, we resolve the conflict between some of the low-level features crucially used
in systems programming and major compiler optimizations.  Specifically, we \emph{develop formal
  semantics} of \emph{relaxed-memory concurrency} (\Cref{chap:relaxed}), \emph{separate compilation}
(\Cref{chap:sepcomp}), and \emph{cast between integers and pointers} (\Cref{chap:intptrcast}) that
$(1)$ supports their common usage patterns and reasoning principles, and $(2)$ provably validates
major compiler optimizations at the same time.

% Developing such a formal semantics requires thorough understanding of the low-level features and
% compiler optimizations because, as we have seen in \Cref{sec:introduction:problem}, it should
% adequately strike the trade-off between them.

% and yet were beyond the reach of the prior work.

Our formal semantics is beneficial to both programmers and compiler writers.  Since formal semantics
is a mathematically clear definition of program behaviors, it makes possible for programmers to
expect how compiled programs will behave regardless of which optimizations are performed by
compilers.  On the other hand, compiler writers can figure out whether an optimization is sound or
not using formal semantics as the criteria.  In particular, with formal semantics, we can point out
which optimization(s) is to blame in the miscompilation bug above.

To establish confidence in our formal semantics, we \emph{prove the soundness of compiler
  optimizations} in the presence of the low-level features.  The soundness proof guarantees that
they preserve the semantics of source programs and do not introduce any bugs.  The absence of
miscompilation bugs ensures higher level of reliability and thus enables optimizations to be used
even for safety-critical systems with confidence.  We have formalized all the soundness proofs
reported in this dissertation in the Coq theorem prover~\cite{coq}, which \emph{automatically and
  rigorously checks} the validity of the soundness proofs.  The formalization is available
online~\cite{kang-phd-thesis-web}.

% In addition to the soundness proofs, most of the key results of this dissertation are also
% formalized in Coq.  


\paragraph*{}

To summarize our contributions, \textbf{we develop formal semantics and verify compiler
  optimizations} for relaxed-memory concurrency (\Cref{chap:relaxed}), separate compilation
(\Cref{chap:sepcomp}), and cast between integers and pointers (\Cref{chap:intptrcast}).  In the rest
of this section, we will explain our main contributions in more details.


\paragraph{\Cref{chap:relaxed}: Relaxed-Memory Concurrency}

Relaxed-memory concurrency is the study of \emph{relaxed behaviors}, which are observable behaviors
of concurrent programs beyond those allowed in \emph{sequential consistency} (think: interleaving of
the executions of threads).  Relaxed behaviors are made possible due to hardware and compiler
optimizations such as out-of-order execution or instruction reordering/merging.  While relaxed
behaviors complicate reasoning of concurrent programs, they are unavoidable in effectively
exploiting the parallelism provided by shared-memory architecture.  Despite many years of research,
however, it has proven very difficult to develop a formal semantics for programming languages with
relaxed-memory concurrency that adequately balances the conflicting desiderata of programmers and
compilers.

In this chapter, we propose the first formal semantics of relaxed-memory concurrency that $(1)$
justifies simple invariant-based reasoning, thus demonstrating the absence of bad
``out-of-thin-air'' behaviors, $(2)$ supports ``DRF'' guarantees, ensuring that programmers who use
sufficient synchronization need not understand the full complexities of relaxed-memory semantics,
$(3)$ is implementable, in the sense that it provably validates many standard compiler optimizations
and reorderings, as well as standard compilation schemes to x86-TSO, $(4)$ accounts for a broad
spectrum of low-level concurrency features in C, and $(5)$ defines the semantics of racy programs
without relying on undefined behaviors, which is a prerequisite for applicability to type-safe
languages like Java.  The key novel idea behind our semantics is the notion of promises: a thread
may make the effect of a write before actually executing it, thus enabling other threads to read
from that write out of order.  Crucially, to prevent out-of-thin-air behaviors, a promise step
requires a thread-local certification that it will be possible to execute the promised write even in
the absence of the promise.

Our semantics draws interest from both industry and academia.  Our semantics not only serves as a
guide to C/C++ relaxed-memory concurrency~\cite{kang-synch-patterns,niko-promising-semantics} but
also influences the discussion on the standard semantics for C/C++ and compiler IRs (private
communication in mailing lists).  Svendsen~\etal{} developed reasoning principles for our
semantics~\cite{promising-logic}, and Podkopaev~\etal{} validated compilation schemes for our
semantics to various architectures such as ARMv7, ARMv8, RISC-V, and Power~\cite{imm-compilation}.
Furthermore, the idea of promises and certification is also used to model the relaxed-memory
concurrency in ARMv8 and RISC-V~\cite{promising-armv8-riscv}.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{promising} \textbf{Jeehoon Kang}, Chung-Kil Hur, Ori Lahav, Viktor
  Vafeiadis, Derek Dreyer.  \emph{A Promising Semantics for Relaxed-Memory Concurrency}.
  \textbf{POPL 2017}.}


\paragraph{\Cref{chap:sepcomp}: Separate Compilation and Linking}

Separate compilation and linking is essential in practice because they significantly reduces
compilation time.  However, major compiler verification efforts, such as CompCert and Vellvm, have
traditionally simplified the verification problem by restricting attention to the correctness of
whole-program compilation, leaving open the question of how to verify the correctness of separate
compilation.  Recently, a number of sophisticated techniques have been proposed for proving more
flexible, compositional notions of compiler correctness, but these approaches tend to be quite
heavyweight compared to the simple ``closed simulations'' used in verifying whole-program
compilation.  Applying such techniques to a compiler like CompCert, as Stewart \etal{} have
done~\cite{stewart+:popl2015}, involves major changes and extensions to its original verification.

In this chapter, we show that if we aim somewhat lower---to prove correctness of separate
compilation, but only for a single compiler---we can drastically simplify the proof effort.  Toward
this end, we develop several lightweight techniques that recast the compositional verification
problem in terms of whole-program compilation, as far as the compiler's transformations and
optimizations satisfy what we call \emph{monotonicity}.  The proof techniques enable us to largely
reuse the closed-simulation proofs from existing compiler verifications.

We demonstrate the effectiveness of these techniques by applying them to CompCert 2.4, converting
its verification of whole-program compilation into a verification of separate compilation in less
than two person-months.  This conversion only required a small number of changes to the original
proofs.  Along the way, we uncovered two compiler bugs---one of which is on separate compilation and
the other is orthogonal to separate compilation---and our proof techniques are subsequently adopted
in CompCert 2.7.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{sepcomp} \textbf{Jeehoon Kang}, Yoonseung Kim, Chung-Kil Hur, Derek Dreyer,
  Viktor Vafeiadis.  \emph{Lightweight Verification of Separate Compilation}.  \textbf{POPL 2016}.}

\paragraph{\Cref{chap:intptrcast}: Cast between Integers and Pointers}

Cast between integers and pointers is one of the defining characteristic of the C programming
language in that it allow cross-platform low-level manipulation of memory layout, which is essential
for applications such as operating system kernels and language runtimes.  However, the feature
drastically conflicts with major compiler optimizations, as demonstrated by the example shown in
\Cref{fig:introduction:bug}.  The ISO C standards try to reconcile the feature and the optimizations
using the notion of \emph{provenance}, but it fails to support certain common optimizations and
requires an intrusive change to the language semantics.

In this chapter, we propose the first formal semantics of cast between integers and pointers that
$(1)$ fully supports operations on the representation of pointers, including all arithmetic
operations for pointers that have been cast to integers, $(2)$ validates major compiler
optimizations on memory accesses, and $(3)$ is simple to understand and program with.  The key novel
idea behind our semantics is the notion of \emph{concretization}: when allocated, a memory block is
not assigned a concrete address yet; only when it is required by a pointer-to-integer cast, the
block is lazily assigned a concrete address, \ie{} the block is concretized.

Along the way, we discovered a GCC bug in the presence of integer-pointer casts, which clearly shows
it is safe to turn off too aggressive alias analyses.  Furthermore, our idea has subsequently been
refined by follow-up papers by other researchers~\cite{intptrcast-oopsla,intptrcast-popl}, which are
accompanied with promising revision proposals to the LLVM IR and the ISO C standard.

% The ISO C standard does not specify the semantics of many valid programs that use non-portable
% idioms such as integer-pointer casts. Recent efforts at formal definitions and verified
% implementation of the C language inherit this feature.  By adopting high-level abstract memory
% models, they validate common optimizations. On the other hand, this prevents reasoning about much
% low-level code relying on the behavior of common implementations, where formal verification has
% many applications.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{intptrcast} \textbf{Jeehoon Kang}, Chung-Kil Hur, William Mansky, Dmitri Garbuzov,
  Steve Zdancewic, Viktor Vafeiadis.  \emph{A Formal C Memory Model Supporting Integer-Pointer
    Casts}.  \textbf{PLDI 2015}.}


\paragraph{Organization}

The rest of this chapter provides the technical background on formal semantics and compiler
verification that informs the rest of this dissertation (\Cref{sec:background}).
\Cref{chap:relaxed,chap:sepcomp,chap:intptrcast} present the main contributions of this
dissertation.  This dissertation concludes with \Cref{chap:epilogue}, which summarizes the main
contributions, the impacts, and future research directions (\Cref{sec:conclusion}).


% formally \emph{understand} and \emph{verify} optimizations as follows:

% In this dissertation, we push forward this research direction---namely, understanding and
% verifying compiler optimizations via formal semantics and soundness proof---to also account for
% three low-level features

% Accordingly, there have been numerous efforts to capture the subtleties of the C standard
% formally, either by giving an alternative language definition or a conforming implementation.

% \paragraph{Prior Art}

% Over ten years ago, Xavier Leroy initiated the CompCert
% project~\cite{compcert:CACM,leroy:compcert}, a landmark effort that
% resulted in the first \emph{realistic verified compiler}.  The
% CompCert compiler~\cite{compcert-website} is \emph{realistic} in the
% sense that it ``could realistically be used in the context of
% production of critical software''.  In particular, it compiles a
% significant subset of the C language down to assembly, and it performs
% a number of common and useful optimizations.  It is \emph{verified} in
% the sense that it ``is accompanied by a machine-checked proof [in Coq]
% of a semantic preservation property: the generated machine code
% behaves as prescribed by the semantics of the source program.''  As
% such, CompCert guarantees that program analyses and verifications
% performed on its input carry over soundly to its machine-level output.
% It has served as a fundamental building block in academic work on
% end-to-end verified software~\cite{appel:plcc}, as well as receiving
% significant interest from the avionics industry~\cite{avionics}.
% % http://projects.laas.fr/IFSE/FMF/J3/slides/P05_Jean_Souyiris.pdf
% % XXX: Any other projects that depend on CompCert besides Appel's
% % XXX: Verified Software Toolchain or Airbus?

% In the same spirit as CompCert, Vellvm~\cite{vellvm} by Steve Zdancewic and his collaborators
% formalizes a significant subset of the LLVM intermediate representation (IR) and verifies
% interesting compiler transformations and optimizations performed at the IR level.  Most notably,
% Vellvm formalizes the static single assignment form (SSA), including a dominance analysis, an SSA
% type checker, and an SSA-aware register promotion algorithm that is simplified from the
% \code{mem2reg} pass in LLVM.

% Because of these simplifications, they are currently suitable only for a niche market such as
% safety-critical embedded systems.

% First, CompCert and Vellvm---while supporting a significant subset of C99 and LLVM IR,
% respectively---lack support for various low-level features that are crucially used in systems
% programming.

% Second, they only perform quite straightforward optimizations that are way less sophisticated than
% those in mainstream compilers.  In short, CompCert and Vellvm provide less control over hardware and
% lower performance than mainstream compilers.

% and support only limited use cases of compilers, \eg{} they did not verify linking.



% However, it is difficult to design a formal semantics for these features that justifies
% optimizations for compilers and supports reasoning principles for programmers because of their
% inherent complexity.  due to subtle pointer aliasing rules~\cite{krebbers2013aliasing}, reliance
% on implementation-specific behavior, the treatment of pointers to uninitialized memory, and
% hardware/compiler optimizations.
% We propose the first such formal semantics that satisfies both compilers and programmers at the same
% time.  Based on our work, revisions to the ISO C standard and mainstream compilers such as GCC and
% LLVM are being prepared.



% \paragraph{Other Papers}

% \begin{itemize}
% % \item[\cite{scfix}] Ori Lahav, Viktor Vafeiadis, \textbf{Jeehoon Kang}, Chung-Kil Hur, Derek Dreyer.
% %   \emph{Repairing Sequential Consistency in C/C++11}.  \textbf{PLDI 2017}.
% \item[\cite{crellvm}] \textbf{Jeehoon Kang}*, Yoonseung Kim*, Youngju Song*, Juneyoung Lee, Sanghoon
%   Park, Mark Dongyeon Shin, Yonghyun Kim, Sungkeun Cho, Joonwon Choi, Chung-Kil Hur, Kwangkeun Yi.
%   \emph{Crellvm: Verified Credible Compilation for LLVM}.  \textbf{PLDI 2018}.  (*The first three
%   authors contributed equally and are listed alphabetically.)
% \end{itemize}



% Specifically, we propose the formal semantics of several low-level features that are the defining
% characteristics of C and yet are omitted from the prior work.  Furthermore, we propose
% verification techniques for real-world compiler use cases.


% by formalizing the semantics of other complex low-level features---such as type-based alias
% analysis, union, inline assembly, GPUs and accelerators---and by verifying mainstream compilers.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
