\section{Introduction}
\label{sec:introduction}

The C programming language is used as the \emph{lingua franca} for systems programming, mainly due
to its three notable advantages: \emph{control}, \emph{portability}, and \emph{performance}.  C
provides precise control over hardware via low-level features such as pointer manipulation,
concurrency, and asynchronous interrupt.  At the same time, C is portable in that C programs can be
compiled and then executed in most of the existing hardware.  Furthermore, C provides decent
performance: a program written in C are usually outperforming equivalent programs written in other
languages---even including carefully hand-written assembly programs---when they are performing
exactly the same task.  These advantages for decades have attracted system programmers, and as a
result, a giant ecosystem was built around the C language.

C enjoys control and portability---seemingly conflicting properties---at the same time because it is
a balanced abstraction over various hardware assembly languages.  If C were exposing too much detail
of hardware, then it would have not been able to support some assembly languages that mismatch with
the exposed detail, losing a significant degree of portability; on the other hand, if C were
exposing too little detail of hardware, then it would have not been able to precisely control over
hardware.  The design choice of C as a hardware abstraction is so popular that other systems
programming languages---such as C++, D, Objective C, Swift, and Rust---largely follow the design of
C and are often called ``C-like''.

% features, losing the opportunity to improve the performance.

% To summarize, the C programming language is an abstraction that should satisfy the desiderata for
% three different ``masters'': portability for programmers, control for hardware, and optimization for
% compilers.

% C---like all the other programming languages---serves multiple ``masters'', namely programmers,
% compilers, and hardware.  From programmer's point of view, C should support \emph{reasoning
%   principles} that are powerful enough to reason about real-world C programs and guarantee their
% safety and functional correctness.  On the other hand, C should validate compiler and hardware
% \emph{optimizations} that may vastly accelerate the execution of C programs and are therefore
% actually performed in the real-world compilers and hardware.  What is particularly interesting about
% C is that ...


\paragraph{Compiler Optimizations}

Being a simple syntax translator from C to assembly languages, however, is insufficient for C
compilers to provide the desired level of performance.  Instead, compilers perform
\emph{optimizations} that transform the given program to be executed more efficiently in hardware
while preserving its semantics.  Optimizations are so effective that they have been an essential
ingredient of compilers since the early days.  For example, every system programmer expects a
compiler to perform quite sophisticated optimizations such as register promotion~\cite{mem2reg} and
register allocation~\cite{regalloc}.  Optimizations are becoming more important these days because
recent hardware trends---such as SIMD, GPU, and accelerators---offer potential for compiler
optimizations to further improve the performance of systems.

The following is an instance of the \emph{constant propagation} optimization, which significantly
improves the performance of compilation results and thus is performed by mainstream compilers such
as GCC~\cite{gcc} and LLVM~\cite{llvm}:

%
\[\begin{array}{rcl}
\begin{minipage}{0.27\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(a);
}
\end{minted}
\end{minipage}
&
\optarrow
&
\begin{minipage}{0.4\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(42); // const. prop.
}
\end{minted}
\end{minipage}
\end{array}\]

\noindent Suppose \code{g()} is an external function whose body is unknown to the compiler, and
\code{print(a)} prints the value of \code{a} to the screen.  The function \code{f()} first assigns
\code{42} to the local variable \code{a} (line \code{1}), calls some unknown external function
\code{g()} (line \code{2}), and then prints \code{a} (line \code{3}).  As an optimization,
mainstream compilers replace \code{a} with \code{42} at line \code{3}, effectively propagating the
constant \code{42} at line \code{1} to line \code{3}.  Compilers perform such an optimization even
in the presence of a function call to the unknown function \code{g()}, because---at least in the
viewpoint of compilers---the address of the variable \code{a} is not leaked to \code{g()} and thus
its content cannot be modified by \code{g()}.

The optimization, however, may change the program's behavior, putting the soundness of the
optimization in danger.  For example, what if \code{f()} is linked with the following \code{g()}?
%
\[
\begin{minipage}{0.8\textwidth}
\begin{minted}{c}
void g() {
1: int anchor;
2: int *guess = &anchor + 10; // guessing &a
3: *guess = 666;
}
\end{minted}
\end{minipage}
\]
%
\noindent Here, the function \code{g()} tries to \emph{guess} the address of \code{a} by exploiting
the fact that stack usually grows downwards with a fixed offset: it first declares a variable
\code{anchor} and guesses that \code{a} is located 10 words later than than \code{anchor} is.  While
this is clearly a bad practice, sometimes the guess happens to be correct, invalidating the
compiler's reasoning that \code{a} is accessible only within the function \code{f()}.  If it is the
case, when linked with \code{g()}, the original \code{f()} will print the evil value 666; on the
other hand, the optimized \code{f()} will print the expected value 42.\footnote{We got these results
  from assembly programs separately compiled from \code{f()} and \code{g()} with GCC 8.2.1 and
  compile option \code{-fno-stack-protector} in an x86-64 machine running Linux 4.18.}

To rescue the soundness of constant propagation, the ISO C18 standard~\cite{c18} blames \code{g()}
for violating the rule of the C language by marking it as invoking \emph{undefined
  behavior}~\cite[\S3.4.3p1]{c18}.  Specifically, line \code{2} invokes undefined behavior because
\code{guess} is derived from \code{anchor} and yet it does not point to the valid location within
\code{anchor}'s allocation~\cite[\S6.5.6p8]{c18}.  (Roughly speaking, all the pointers derived from
\code{anchor} \emph{shall} point to \code{anchor}; otherwise, the behavior is undefined.)  Now an
instance of undefined behavior allows compilers to do anything it chooses, from arbitrarily changing
the code to even making ``demons fly out of your nose''~\cite{nasal-demons}, justifying the constant
propagation optimization.\footnote{Notice that undefined behavior is not necessary for higher-level
  languages---such as Java, C\#, OCaml, Haskell---to justify compiler optimizations thanks to their
  lack of precise control over low-level details of hardware.  For example, constant propagation is
  immediately justified in Haskell without resorting to undefined behavior thanks to its lack of raw
  pointer.}

% The compiler's reasoning, however, seems wrong in certain circumstances, 


\paragraph{Trading Control for Performance}

In the course of doing so, C is effectively trading control for performance.  The C language
intentionally loses precise control over memory layout by forbidding arbitrary pointer
manipulation---\eg{} deriving the address of \code{a} from that of \code{anchor}---in order to
justify the constant propagation optimization, which significantly improves the performance of
compilation results.  As a result, pointers in C have a richer structure than those in assembly
languages, which have the same representation as integer values of the appropriate width and simply
index into a single flat array representing memory.

This practice of trading control for performance is actually quite common in the design of C, and as
a consequence, C significantly deviates from assembly languages.  Mainstream compilers are
performing aggressive optimizations that change the behaviors of programs that use low-level
features, defeating the intention of programmers to precisely control over hardware.  As a result, C
programs often have vastly different meaning than the exactly same programs written in assembly
languages (modulo syntactic differences), and thus C is not just a thin wrapper around assembly
languages but rather should be an abstraction over both assembly languages \emph{and} compiler
optimizations.

% Therefore C have richer structures than those in assembly languages due to compiler optimizations.

% The above transformation clearly shows that C is a very different language from assembly languages:
% the constant propagation optimization should sound for C programs---as it is performed by mainstream
% compilers---but it is unsound for assembly programs.

% Now suppose that \code{f()} in the above example were assembly programs (modulo syntactic
% differences).  Then the transformation would be incorrect in certain circumstances.

% threaten the portability of C programs by changing the source program's behavior in certain
% circumstances.

% suppose that \code{g()} is adversarially trying to \emph{guess} the address of \code{a} as follows:

% Since optimizations are an essential ingredient of the real-world practice of C programming, the
% language should be

% system programmers are building bigger systems, which they cannot hand-optimize on their own; and
% hardware vendors are introducing complex features, which need a special attention for maximal
% utilization.

% The constant propagation example above shows that the low-level control over memory layout via
% pointer manipulation conflicts with a simple compiler optimization, and C resolves the conflict by
% marking the accesses to the outside of a memory allocation's valid range as invoking undefined
% behavior.


\subsection{Problem: Unclear Specification of Compiler Optimizations}
\label{sec:introduction:problem}

Abstracting over assembly languages and compiler optimizations, however, is a challenging task due
to the conflict between control and performance.  On the one hand, the C language should support
low-level features that are capable of precisely controlling the hardware, such as pointer
manipulation, concurrency, and asynchronous interrupt; on the other hand, C should support
sophisticated and yet effective optimizations performed by mainstream compilers, such as register
promotion and register allocation.  Supporting both low-level features and sophisticated
optimizations has proven very difficult, at least for some cases, despite many years of research and
ISO standardization efforts~\cite{c18}.

As a consequence, the C programming language have quite unclear specification.  The state-of-the-art
ISO C standard, even after a series of revisions including C89, C99, C11, and C18, informally
describes the language in English with many ad-hoc exceptions including 203 cases of undefined
behavior~\cite[J.2]{c18}.  The informal description is often ambiguous and confusing, especially for
corner cases.  Thus many systems programming communities, \eg{} the Linux kernel developers, use
their own \emph{dialect} of C that is closer to assembly languages, supporting more low-level
features and less compiler optimizations than the standard.  The dialect is often described
informally as the set of turned-on compiler optimizations, whose meaning is unstable and changing.

The unclear specification of the language causes difficulties to both programmers and compiler
writers.  For programmers, it is difficult to expect how compiled programs will behave---especially
when they use low-level features---because optimizations and their combinations often introduce
surprising non-local changes and difficult-to-find bugs in program
behavior~\cite{wang2013towards,yang2011finding}.  One of the consequences is that mainstream
compilers are typically unused for safety-critical systems, and even when it is used, programmers
turn off most compiler optimizations, significantly increasing verification cost and degrading
performance of safety-critical systems.  One the other hand, for compiler writers, it is difficult
to figure out whether an optimization is sound or not.  Even worse, sometimes a combination of
optimizations---each of which seems legit---results in miscompilation bugs, for which it is unclear
which optimizations are to blame.

% in particular, those bugs are not effectively found and fix by testing due to unclear specification.

% For instance, compilers reason backwards from instances of undefined behavior to conclude that,
% \eg{} certain code paths must be dead and to remove the code.  for example, certain code paths
% must be dead and to remove those paths.  to conclude that,

% To improve performance and energy consumption of systems, compiler writers have introduced dozens
% of subtle optimizations even though their soundness is justified solely by intuition.

% This kind of conflicts and resolutions (using undefined behavior) are common in the design of the
% C programming language semantics.  For example, the example above shows that low-level control
% over memory layout via pointer manipulation in C conflicts with a simple compiler optimization,
% and the conflict is resolved by marking the accesses to the outside of a memory allocation's valid
% range as invoking undefined behavior.  More generally, the C language has many conflicts between
% control over various low-level details of hardware and various compiler optimizations implemented
% in mainstream C compilers, and these conflicts are usually resolved by marking some unintended
% usage of language features as invoking undefined behavior.

% The ISO C18 standard~\cite{c18} famously does not give semantics to a significant subset of
% syntactically valid C programs. Instead, many programs exhibit implementation-defined, unspecified,
% or undefined behavior, with the latter imposing no requirements on a conforming implementation.


\paragraph{A Miscompilation Bug}

\begin{figure}[t]
\begin{center}
\small
\begin{minipage}{0.5\textwidth}
\begin{minted}[bgcolor=white]{c}
void main() { 
  int x = 0;
  uintptr_t xi = (uintptr_t) &x;
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=gray]{c}
  uintptr_t i;
  for (i = 0; i < xi; ++i) {}
  if (xi != i) {
    printf("unreachable\n");
    xi = i;
  }
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=white]{c}
  auto p = (int *) xi;
  *p = 1;
  printf("%d\n", x); // expected: 1, actual: 0
}
\end{minted}
\end{minipage}
\end{center}
\caption{A GCC bug in the presence of integer-pointer casts}
\label{fig:introduction:bug}
\end{figure}

\Cref{fig:introduction:bug} presents a GCC miscompilation bug we found due to conflicting
optimizations.\footnote{We reported this miscompilation bug as a comment in
  \url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=65752}, which is still open as of this writing.}
(At first, ignore the gray area.)  Note that the type \code{uintptr\_t} is an integer type that is
able to hold a pointer value~\cite[\S7.20.1.4]{c18}.  In the program, the pointer to the local
variable \code{x} is cast to an integer, \code{xi}, and then cast back to pointer, \code{p}.  Thus
\code{p} points to \code{x}, and after \code{*p} is assigned one, the value of \code{x} should be
one.  Now inserting the code in the gray area should not change the program's behavior, because the
gray area is dead code: after the for loop, \code{i} equals to \code{xi}, and the conditional branch
is not taken.  But GCC miscompiles the program as follows:
%
\begin{enumerate}
\item Code motion optimization moves \mintinline{c}{xi = i} out of the conditional branch, because
  regardless of whether the branch is taken, \code{xi = i} should hold.
\item Alias analysis thinks that \code{p} is not a valid pointer, because it originates from
  \code{xi}, which equals to \code{i}, which in turn is obtained by just incrementing by one several
  times from zero.
\item Constant propagation optimization replaces \code{x} at the last line with zero, because
  \code{x} is initialized with zero and no intervening stores are writing to \code{x}.  In
  particular, \code{p} is not aliased with \code{\&x}, at least from the compiler's point of view,
  because \code{p} is not a valid pointer.
\end{enumerate}

\noindent It is unclear which optimizations or analyses involved with this bug are to blame, because
all of them look fine: the code motion and constant propagation optimizations seem legit, and the
alias analysis has a reasonable cause to believe that \code{p} is invalid.  But still, they are
conflicting with each other and they collectively result in a miscompilation bug.



\subsection{Research Direction: Understanding and Verifying Optimizations}

In order to address the difficulties caused by the unclear specification of compiler optimizations,
researchers have proposed to \emph{understand} and \emph{verify} optimizations as follows:

\begin{itemize}
\item \textbf{Understanding optimizations with formal
    semantics}~\cite{norrish1998c,leroy:compcert,ellison2012executable}: formal semantics is a
  mathematically clear definition of program behaviors---as opposed to informal English prose as in
  the ISO C18 standard---that completely removes the ambiguity and confusion in the semantics.
  Formal semantics is the result of thorough understanding of compiler optimizations, as it should
  adequately strike the balance between control and performance.  Formal semantics informs
  programmers exactly how programs will behave and how they can precisely control over the hardware;
  at the same time, it informs compiler writers which optimizations are sound.  For instance, formal
  semantics should point out which optimizations are to blame in the miscompilation bug above.

\item \textbf{Verifying optimizations with soundness proof}~\cite{compcert,vellvm}: verified
  optimizations are accompanied with \emph{machine-checked} soundness proof, which formally
  guarantees that the optimizations preserve the semantics of source programs and do not introduce
  any bugs.  In order to verify sophisticated optimizations performed by mainstream compilers, it is
  necessary to develop powerful proof techniques.  Verification ensures very high-level of
  reliability for compiler optimizations---\ie{} the absence of miscompilation bugs---so that they
  can be used for safety-critical systems with confidence.

\end{itemize}

% Accordingly, there have been numerous efforts to capture the subtleties of the C standard
% formally, either by giving an alternative language definition or a conforming implementation.

\paragraph{Prior Art}

A landmark in this research direction is the CompCert C compiler~\cite{compcert}, which was
initiated by Xavier Leroy over ten years ago and then grows as the first realistic verified
compiler.  CompCert is verified in the sense that it ``is accompanied by a machine-checked proof of
a semantic preservation property: the generated machine code behaves as prescribed by the semantics
of the source program.''  As such, CompCert guarantees that program analyses and verifications
performed on its input carry over soundly to its machine-level output.  CompCert is realistic in the
sense that it ``could realistically be used in the context of production of critical software''.  It
compiles a significant subset of ISO C99 down to several architectures, and it performs a number of
common and useful optimizations.  It received significant interest from the avionics
industry~\cite{TODO}, and recently, it is certified for being used for nuclear power
plant~\cite{compcert-nuclear}.  It has also served as a fundamental building block in academic work
on end-to-end verified software~\cite{TODO}.

% Over ten years ago, Xavier Leroy initiated the CompCert
% project~\cite{compcert:CACM,leroy:compcert}, a landmark effort that
% resulted in the first \emph{realistic verified compiler}.  The
% CompCert compiler~\cite{compcert-website} is \emph{realistic} in the
% sense that it ``could realistically be used in the context of
% production of critical software''.  In particular, it compiles a
% significant subset of the C language down to assembly, and it performs
% a number of common and useful optimizations.  It is \emph{verified} in
% the sense that it ``is accompanied by a machine-checked proof [in Coq]
% of a semantic preservation property: the generated machine code
% behaves as prescribed by the semantics of the source program.''  As
% such, CompCert guarantees that program analyses and verifications
% performed on its input carry over soundly to its machine-level output.
% It has served as a fundamental building block in academic work on
% end-to-end verified software~\cite{appel:plcc}, as well as receiving
% significant interest from the avionics industry~\cite{avionics}.
% % http://projects.laas.fr/IFSE/FMF/J3/slides/P05_Jean_Souyiris.pdf
% % XXX: Any other projects that depend on CompCert besides Appel's
% % XXX: Verified Software Toolchain or Airbus?

In the same spirit as CompCert, Vellvm~\cite{vellvm} by Steve Zdancewic and his collaborators
formalizes a significant subset of the LLVM intermediate representation (IR) and verifies
interesting compiler transformations and optimizations performed at the IR level.  Most notably,
Vellvm formalizes the static single assignment form (SSA), including a dominance analysis, an SSA
type checker, and an SSA-aware register promotion algorithm that is simplified from the
\code{mem2reg} pass in LLVM.

% Because of these simplifications, they are currently suitable only for a niche market such as
% safety-critical embedded systems.

However, these projects make unrealistic simplifying assumptions on C semantics and compilers,
skating over the complexity of the real-world practice of systems programming.  First, CompCert and
Vellvm---while supporting a significant subset of C99 and LLVM IR, respectively---provide less
control over hardware than mainstream compilers in that they lack support for various low-level
features that are crucially used in systems programming, such as memory layout, concurrency, and
linking.  Second, CompCert and Vellvm generate less efficient assembly programs than mainstream
compilers in that they only perform quite straightforward optimizations that are way less
sophisticated than those in mainstream compilers.

% and support only limited use cases of compilers, \eg{} they did not verify linking.


\subsection{Our Contribution: Pushing Forward to the Real-World Practice}

In this dissertation, we push forward the research direction to also account for three low-level
features that are crucially used in systems programming and yet were beyond the reach of the prior
work.  Specifically, we design formal semantics for \emph{casts between integers and pointers}
(\Cref{chap:intptrcast}), \emph{relaxed-memory concurrency} (\Cref{chap:relaxed}), and
\emph{linking} (\Cref{chap:sepcomp}), and develop proof techniques for verifying compiler
optimizations in the presence of those features.  To establish confidence in our contributions, we
have formalized most of the key results in the Coq theorem prover~\cite{coq}.  The formalization is
available online~\cite{kang-phd-thesis-web}.

In the rest of this section, we will explain the three features and our contributions in more
details.

% However, it is difficult to design a formal semantics for these features that justifies
% optimizations for compilers and supports reasoning principles for programmers because of their
% inherent complexity.  due to subtle pointer aliasing rules~\cite{krebbers2013aliasing}, reliance
% on implementation-specific behavior, the treatment of pointers to uninitialized memory, and
% hardware/compiler optimizations.
% We propose the first such formal semantics that satisfies both compilers and programmers at the same
% time.  Based on our work, revisions to the ISO C standard and mainstream compilers such as GCC and
% LLVM are being prepared.


\paragraph{\Cref{chap:relaxed}: Relaxed-Memory Concurrency}

\jeehoon{redo it.  Discuss importance and prior work.} It has proven very difficult to develop a
formal semantics for concurrent programming languages that adequately balances the conflicting
desiderata of programmers and compilers.

In this chapter, we propose the first formal semantics of relaxed-memory concurrency that $(1)$
justifies simple invariant-based reasoning, thus demonstrating the absence of bad
``out-of-thin-air'' behaviors, $(2)$ supports ``DRF'' guarantees, ensuring that programmers who use
sufficient synchronization need not understand the full complexities of relaxed-memory semantics,
$(3)$ is implementable, in the sense that it provably validates many standard compiler optimizations
and reorderings, as well as standard compilation schemes to x86-TSO, $(4)$ accounts for a broad
spectrum of low-level concurrency features in C, and $(5)$ defines the semantics of racy programs
without relying on undefined behaviors, which is a prerequisite for applicability to type-safe
languages like Java.  The key novel idea behind our semantics is the notion of promises: a thread
may promise to execute a write in the future, thus enabling other threads to read from that write
out of order.  Crucially, to prevent out-of-thin-air behaviors, a promise step requires a
thread-local certification that it will be possible to execute the promised write even in the
absence of the promise.

\jeehoon{redo it.}  Our semantics is a promising proposal for relaxed-memory concurrency
semantics for C/C++ and compiler IRs, and draws interest from both industry and academia.  In
particular, our semantics serves as a guide to C/C++ relaxed-memory concurrency, and reasoning
principles for our semantics are being developed.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{promising} \textbf{Jeehoon Kang}, Chung-Kil Hur, Ori Lahav, Viktor
  Vafeiadis, Derek Dreyer.  \emph{A Promising Semantics for Relaxed-Memory Concurrency}.
  \textbf{POPL 2017}.}



\paragraph{\Cref{chap:sepcomp}: Separate Compilation and Linking}

Separate compilation and linking is essential in practice because they significantly reduces
compilation time.  However, major compiler verification efforts, such as CompCert and Vellvm, have
traditionally simplified the verification problem by restricting attention to the correctness of
whole-program compilation, leaving open the question of how to verify the correctness of separate
compilation.  Recently, a number of sophisticated techniques have been proposed for proving more
flexible, compositional notions of compiler correctness, but these approaches tend to be quite
heavyweight compared to the simple ``closed simulations'' used in verifying whole-program
compilation.  Applying such techniques to a compiler like CompCert, as Stewart \etal{} have done,
involves major changes and extensions to its original verification.

In this chapter, we show that if we aim somewhat lower---to prove correctness of separate
compilation, but only for a single compiler---we can drastically simplify the proof effort.  Toward
this end, we develop several lightweight techniques that recast the compositional verification
problem in terms of whole-program compilation, thereby enabling us to largely reuse the
closed-simulation proofs from existing compiler verifications.

We demonstrate the effectiveness of these techniques by applying them to CompCert 2.4, converting
its verification of whole-program compilation into a verification of separate compilation in less
than two person-months.  This conversion only required a small number of changes to the original
proofs.  We uncovered two compiler bugs along the way---one of which is on separate compilation and
the other is orthogonal to separate compilation---and our verification techniques are subsequently
adopted in CompCert 2.7.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{sepcomp} \textbf{Jeehoon Kang}, Yoonseung Kim, Chung-Kil Hur, Derek Dreyer,
  Viktor Vafeiadis.  \emph{Lightweight Verification of Separate Compilation}.  \textbf{POPL 2016}.}

\paragraph{\Cref{chap:intptrcast}: Cast between Integers and Pointers}

Cast between integers and pointers is one of the defining characteristic of the C programming
language in that it allow cross-platform low-level access to memory, which is essential for
applications such as operating system kernels and language runtimes.  However, the feature
drastically conflicts with major compiler optimizations, as demonstrated by the example shown in
\Cref{fig:introduction:bug}.  The ISO C standards try to reconcile the feature and the optimizations
using the notion of \emph{provenance}, but it fails to support certain common optimizations and
requires an intrusive change to the language semantics.

In this chapter, we propose the first formal semantics of cast between integers and pointers that
$(1)$ fully supports operations on the representation of pointers, including all arithmetic
operations for pointers that have been cast to integers, $(2)$ validates major compiler
optimizations on memory accesses, and $(3)$ is simple to understand and program with.  The key novel
idea behind our semantics is the notion of \emph{concretization}: when allocated, a memory block is
not assigned a concrete address yet; only when it is required by a pointer-to-integer cast, the
block is lazily assigned a concrete address, \ie{} the block is concretized.

In the course of doing this research, we discovered the GCC bug presented in
\Cref{fig:introduction:bug}, with which we persuaded compiler writers that it is safe to turn off
too aggressive alias analyses.  Furthermore, our idea has subsequently been refined by follow-up
papers by other researchers~\cite{intptrcast-oopsla,intptrcast-popl}, which are accompanied with
promising revision proposals to the LLVM compiler and ISO C standard.


% The ISO C standard does not specify the semantics of many valid programs that use non-portable
% idioms such as integer-pointer casts. Recent efforts at formal definitions and verified
% implementation of the C language inherit this feature.  By adopting high-level abstract memory
% models, they validate common optimizations. On the other hand, this prevents reasoning about much
% low-level code relying on the behavior of common implementations, where formal verification has
% many applications.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{intptrcast} \textbf{Jeehoon Kang}, Chung-Kil Hur, William Mansky, Dmitri Garbuzov,
  Steve Zdancewic, Viktor Vafeiadis.  \emph{A Formal C Memory Model Supporting Integer-Pointer
    Casts}.  \textbf{PLDI 2015}.}

\paragraph{Organization}

The rest of dissertation is organized as follows.  The rest of this chapter provides the technical
background on CompCert that informs the rest of this dissertation (\Cref{sec:background}).
\Cref{chap:relaxed,chap:sepcomp,chap:intptrcast} present the main contributions of this
dissertation.  This dissertation concludes with \Cref{chap:epilogue}, which summarizes the main
contributions, the impacts, and future research directions (\Cref{sec:conclusion}).

% Specifically, we propose the formal semantics of several low-level features that are the defining
% characteristics of C and yet are omitted from the prior work.  Furthermore, we propose
% verification techniques for real-world compiler use cases.


% by formalizing the semantics of other complex low-level features---such as type-based alias
% analysis, union, inline assembly, GPUs and accelerators---and by verifying mainstream compilers.


% Other papers:
%
% \begin{itemize}
% \item[\cite{scfix}] Ori Lahav, Viktor Vafeiadis, \textbf{Jeehoon Kang}, Chung-Kil Hur, Derek Dreyer.
%   \emph{Repairing Sequential Consistency in C/C++11}.  \textbf{PLDI 2017}.
% \item[\cite{crellvm}] \textbf{Jeehoon Kang}*, Yoonseung Kim*, Youngju Song*, Juneyoung Lee, Sanghoon
%   Park, Mark Dongyeon Shin, Yonghyun Kim, Sungkeun Cho, Joonwon Choi, Chung-Kil Hur, Kwangkeun Yi.
%   (*The first three authors contributed equally and are listed alphabetically.)  \emph{Crellvm:
%     Verified Credible Compilation for LLVM}.  \textbf{PLDI 2018}.
% \end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
