\section{Introduction}
\label{sec:introduction}

\subsection{The Context: The C Programming Language}

\paragraph{A Hardware Abstraction}

The C programming language is the \emph{lingua franca} for systems programming, mainly due to its
two notable advantages: \emph{portability} and \emph{control} over hardware.  C is portable in that
C programs can be compiled and then executed in most of the existing hardware.  At the same time, C
has a precise control over hardware in that C allows programmers to access low-level hardware
features such as memory layout and concurrency.  These advantages have attracted system programmers
for decades, resulting in a giant ecosystem around the C language itself and tools such as
optimizing compilers, linkers, and program analyzers.

C enjoys portability and control---seemingly conflicting properties---at the same time thanks to the
fact that it is a balanced abstraction over various hardware assembly languages.  If C were exposing
too much detail of hardware, then it would have not been able to support some assembly languages
that mismatch with the exposed details, losing a significant degree of portability; on the other
hand, if C were exposing too little detail of hardware, then it would have lost the precise control
over them.  The design choice of C as a hardware abstraction is so popular that other systems
programming languages---such as C++, D, Objective C, Swift, and Rust---largely follow the design of
C and are often called ``C-like''.

% To summarize, the C programming language is an abstraction that should satisfy the desiderata for
% three different ``masters'': portability for programmers, control for hardware, and optimization for
% compilers.

% C---like all the other programming languages---serves multiple ``masters'', namely programmers,
% compilers, and hardware.  From programmer's point of view, C should support \emph{reasoning
%   principles} that are powerful enough to reason about real-world C programs and guarantee their
% safety and functional correctness.  On the other hand, C should validate compiler and hardware
% \emph{optimizations} that may vastly accelerate the execution of C programs and are therefore
% actually performed in the real-world compilers and hardware.  What is particularly interesting about
% C is that ...


\paragraph{Compiler Optimization and Undefined Behavior}

However, C is not just a thin wrapper around assembly languages because of compiler optimizations.
They have been so crucial for the performance of systems since the early days that every system
programmer expect a compiler to perform, \eg{} register promotion and register
allocation~\cite{reg-prom, reg-alloc}, which are very effective and yet quite sophisticated compiler
optimizations.  Optimizations are becoming more and more important these days because recent trends
offer potential for them to further improve the performance of systems.  Since they are an essential
ingredient of the real-world practice of C programming, the language should be an abstraction not
only over just hardware assembly languages but also over compiler optimizations.

% system programmers are building bigger systems, which they cannot hand-optimize on their own; and
% hardware vendors are introducing complex features, which need a special attention for maximal
% utilization.

% These days the mainstream compilers are becoming so aggressive in these days that they are
% performing even subtle optimizations that cannot be immediately justified.

Some compiler optimizations are so subtle that they cannot be justified without an artifact.  For
example, consider the following optimization that is actually performed by the mainstream compilers
such as GCC~\cite{gcc} and LLVM~\cite{llvm}:

\[\begin{array}{rcl}
\begin{minipage}{0.27\textwidth}
\begin{minted}{c}
void f() {
  10: int x = 42;
  20: g();
  30: print(x);
}
\end{minted}
\end{minipage}
&
\optarrow
&
\begin{minipage}{0.4\textwidth}
\begin{minted}{c}
void f() {
  10: int x = 42;
  20: g();
  30: print(42); // const. prop.
}
\end{minted}
\end{minipage}
\end{array}\]

\noindent Suppose \code{g()} is an external function whose body is unknown to the compiler, and
\code{print($x$)} prints the value of $x$ to the screen.  The function \code{f()} first assigns
\code{42} to \code{x} (line \code{10}), calls \code{g()} (line \code{20}), and then prints \code{x}
(line \code{30}).  Mainstream compilers replaces \code{x} with \code{42} at line \code{30},
effectively propagating the constant \code{42} at line \code{10} to line \code{30}.  Compilers
perform such a \emph{constant propagation} optimization because they analyze that the variable
\code{x} is accessible only within the function \code{f()}, since its address is not leaked to
elsewhere, and thus \code{g()} cannot modify the content of \code{x}.

But what if an adversarial \code{g()} tries to \emph{guess} the address of \code{x} as follows?
%
\[
\begin{minipage}{0.8\textwidth}
\begin{minted}{c}
void g() {
  10: int anchor;
  20: int *guess = &anchor + 10; // guessing &x
  30: *guess = 666;
}
\end{minted}
\end{minipage}
\]
%
\noindent The function \code{g()} tries to guess the address of \code{x} by exploiting the fact that
stack usually grows downwards: it first declares a variable \code{anchor}, and guesses that \code{x}
is located 10 words later than than \code{anchor} is.  Sometimes, the guess happens to be correct
when compiled with GCC: when the adversarial \code{g()} is linked with the original \code{f()},
\code{f()} will surprisingly print the evil value 666; on the other hand, when \code{g()} is linked
with the optimized \code{f()}, \code{f()} will print the propagated value 42 as
expected.\footnote{We got this result with GCC 8.2.1 and compile option \code{-fno-stack-protector}
  in Linux 4.18.} This example invalidates the compiler's analysis that \code{x} is accessible only
within the function \code{f()}, putting the soundness of the optimization in danger.

In order to rescue the soundness of constant propagation, C blames the adversarial \code{g()} by
marking it as invoking \emph{undefined behavior}~\cite{undefined-behavior}: \code{g()} is not
following the rule of C so that compilers can do anything it chooses, even ``to make demons fly out
of your nose''~\cite{nasal-demons}.  Specifically, \code{g()} invokes undefined behavior in the ISO
C18 standard~\cite{c18} because the pointer \code{guess} is outside of the valid range of the memory
allocation of \code{anchor}, from which $\code{guess}$ is derived from, and thus \code{guess} is an
invalid address~\cite{c11-6.5.6p8}.  In other words, compilers may safely assume that all the
pointers derived from \code{anchor} \emph{shall} point to \code{anchor}, because a failure to
conform with such an assumption invokes undefined behavior.

What is particularly interesting about C is the widespread use of undefined behavior in defining
language semantics.  The constant propagation example above shows that the low-level control over
memory layout via pointer manipulation conflicts with a simple compiler optimization, and C resolves
the conflict by marking adversarial programs as invoking undefined behavior.  C typically applies
this strategy for resolving the conflicts between control over various low-level details of hardware
and various compiler optimizations implemented in mainstream C compilers, introducing a lot of
undefined behavior instances in the language semantics.  It is worth comparing C with higher-level
languages---such as Java, C\#, OCaml, Haskell---that do not need undefined behaviors to justify
compiler optimizations thanks to their lack of precise control over low-level details of hardware,
\eg{} constant propagation is immediately justified in Haskell without resorting to undefined
behavior thanks to its lack of raw pointer.


\subsection{The Problem: Reckless Development of Semantics and Compilers}

The problem is that the C language and its compilers have evolved in such an unplanned way that even
experienced system programmers disagree on the semantics of several language features and the
soundness of various compiler optimizations.  In order to improve performance and energy consumption
of systems, compiler writers have introduced dozens of subtle optimizations even though their
soundness is justified solely by intuition; in turn, to justify those optimizations afterwards, the
recent ISO C standards mark certain programs as invoking undefined behavior with a variety of ad-hoc
exceptions, making the already informal semantics more ambiguous and confusing.  In order to
mitigate the problem caused by reckless development of C semantics and compilers, ISO revises the C
semantics in a series of standards---C89, C99, C11, and C18---but they are still complex and are not
widely accepted in the systems programming community, \eg{} the Linux community defines its own
dialect of C that supports much less compiler optimizations and is closer to the assembly language
than ISO C18.

TODO: explain the problems in more details.  Too many undefined behaviors, conflicts among
optimizations.  ISO C18 is English prose.


\subsection{The Prior Art: Formal Semantics and Compiler Verification}

In order to systematically address the problems caused by the unplanned evolution of C semantics and
compilers, researchers have proposed to \emph{define the formal semantics} of C and \emph{prove the
  soundness of compiler optimizations} w.r.t. the formal semantics.  In this research agenda, we
describe the C semantics no longer in an informal English prose (as ISO C18 does) but in a
mathematically clear formalization, thereby completely removing the ambiguity in the semantics;
furthermore, based on the formalized semantics, we prove that compiler optimizations preserve the
semantics of source program, conclusively vindicating them from miscompilation bugs.

A landmark in this agenda is the CompCert C compiler~\cite{compcert}, which was initiated by Xavier
Leroy over ten years ago and grows as the first realistic verified compiler.  The CompCert compiler
is realistic in the sense that it ``could realistically be used in the context of production of
critical software''.  In particular, it compiles a significant subset of ISO C99 down to assembly,
and it performs a number of common and useful optimizations.  It is verified in the sense that it
``is accompanied by a machine-checked proof [in Coq] of a semantic preservation property: the
generated machine code behaves as prescribed by the semantics of the source program.''  As such,
CompCert guarantees that program analyses and verifications performed on its input carry over
soundly to its machine-level output.  It has served as a fundamental building block in academic work
on end-to-end verified software~\cite{TODO}, as well as receiving significant interest from the
avionics industry~\cite{TODO}.

In the same spirit as CompCert, Vellvm~\cite{vellvm} by Steve Zdancewic and his collaborators
formalizes a significant subset of the LLVM IR and verifies interesting compiler transformations and
optimizations performed at the IR level.  Most notably, Vellvm formalizes the static single
assignment form (SSA), including a dominance analysis, an SSA type checker, and an SSA-aware
register promotion algorithm that is simplified from the \code{mem2reg} pass in LLVM.

However, these projects make big simplifying assumptions on C semantics and compilers, skating over
the complexity of the real-world practice of systems programming.  While CompCert and Vellvm support
a significant subset of C99 and LLVM IR, respectively, they lack support for various low-level
features that are crucially used in many system programs, such as memory layout, concurrency, and
processor register manipulation.  Furthermore, CompCert and Vellvm only perform quite
straightforward transformations and optimizations that are way less sophisticated than those in
mainstream compilers such as GCC and LLVM, and support only limited use cases of compilers, \eg{}
they did not verify linking.  Because of these simplifications, CompCert and Vellvm are currently
suitable only for a niche market such as safety-critical embedded systems.


\subsection{Our Contribution: Towards Formalization of C in the Wild}

In this dissertation, we carry forward the research agenda for the formalization of real-world
practice of C semantics and compilers by lifting the prior work's simplifying assumptions in several
dimensions.  Specifically, we make the following contributions:

% Specifically, we propose the formal semantics of several low-level features that are the defining
% characteristics of C and yet are omitted from the prior work.  Furthermore, we propose
% verification techniques for real-world compiler use cases.

%
\paragraph{Cast between Integers and Pointers}

While cast between integers and pointers is one of the defining characteristics of the C programming
language, the feature has not been formalized in CompCert and Vellvm because it drastically
conflicts with major compiler optimizations.  The ISO C standards try to reconcile the feature and
the optimizations using the notion of \emph{provenance}, but it fails to support certain common
optimizations and requires an intrusive change to the language semantics.  In
\Cref{chap:intptrcast}, we propose the first formal semantics of casts between integers and pointers
that $(1)$ fully supports operations on the representation of pointers, including all arithmetic
operations for pointers that have been cast to integers, $(2)$ validates major compiler
optimizations on memory accesses, and $(3)$ is simple to understand and program with.

The key novel idea behind our semantics is the notion of concretization: when allocated, a memory
block is not assigned a concrete address yet; only when it is required by a pointer-to-integer cast,
the block is lazily assigned a concrete address, \ie{} the block is concretized.

TODO: impact.

% The ISO C standard does not specify the semantics of many valid programs that use non-portable
% idioms such as integer-pointer casts. Recent efforts at formal definitions and verified
% implementation of the C language inherit this feature.  By adopting high-level abstract memory
% models, they validate common optimizations. On the other hand, this prevents reasoning about much
% low-level code relying on the behavior of common implementations, where formal verification has
% many applications.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{intptrcast} \textbf{Jeehoon Kang}, Chung-Kil Hur, William Mansky, Dmitri Garbuzov,
  Steve Zdancewic, Viktor Vafeiadis.  \emph{A Formal C Memory Model Supporting Integer-Pointer
    Casts}.  \textbf{PLDI 2015}.}

\paragraph{Relaxed-Memory Concurrency}

Despite many years of research, it has proven very difficult to develop a formal semantics for
concurrent programming languages that adequately balances the conflicting desiderata of programmers,
compilers, and hardware.  In \Cref{chap:promising}, we propose the first formal semantics of
relaxed-memory concurrency that $(1)$ accounts for a broad spectrum of low-level concurrency
features in C, $(2)$ is implementable, in the sense that it provably validates many standard
compiler optimizations and reorderings, as well as standard compilation schemes to x86-TSO and
Power, $(3)$ justifies simple invariant-based reasoning, thus demonstrating the absence of bad
``out-of-thin-air'' behaviors, $(4)$ supports ``DRF'' guarantees, ensuring that programmers who use
sufficient synchronization need not understand the full complexities of relaxed-memory semantics,
and $(5)$ defines the semantics of racy programs without relying on undefined behaviors, which is a
prerequisite for applicability to type-safe languages like Java.

The key novel idea behind our semantics is the notion of promises: a thread may promise to execute a
write in the future, thus enabling other threads to read from that write out of order.  Crucially,
to prevent out-of-thin-air behaviors, a promise step requires a thread-local certification that it
will be possible to execute the promised write even in the absence of the promise.

TODO: impact.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{promising} \textbf{Jeehoon Kang}, Chung-Kil Hur, Ori Lahav, Viktor
  Vafeiadis, Derek Dreyer.  \emph{A Promising Semantics for Relaxed-Memory Concurrency}.
  \textbf{POPL 2017}.}


\paragraph{Lightweight Verification of Separate Compilation}

Major compiler verification efforts, such as CompCert and Vellvm, have traditionally simplified the
verification problem by restricting attention to the correctness of whole-program compilation,
leaving open the question of how to verify the correctness of separate compilation.  Recently, a
number of sophisticated techniques have been proposed for proving more flexible, compositional
notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the
simple ``closed simulations'' used in verifying whole-program compilation.  Applying such techniques
to a compiler like CompCert, as Stewart \etal{} have done, involves major changes and extensions to
its original verification.

In \Cref{chap:sepcomp}, we show that if we aim somewhat lower---to prove correctness of separate
compilation, but only for a single compiler---we can drastically simplify the proof effort.  Toward
this end, we develop several lightweight techniques that recast the compositional verification
problem in terms of whole-program compilation, thereby enabling us to largely reuse the
closed-simulation proofs from existing compiler verifications.  We demonstrate the effectiveness of
these techniques by applying them to CompCert 2.4, converting its verification of whole-program
compilation into a verification of separate compilation in less than two person-months.  This
conversion only required a small number of changes to the original proofs, and uncovered two
compiler bugs along the way.

Our verification techniques are subsequently adopted in CompCert 2.7.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{sepcomp} \textbf{Jeehoon Kang}, Yoonseung Kim, Chung-Kil Hur, Derek Dreyer,
  Viktor Vafeiadis.  \emph{Lightweight Verification of Separate Compilation}.  \textbf{POPL 2016}.}

\paragraph*{}

To establish confidence in all our contributions, we have formalized most of our key results in Coq.
The formalization is available at \url{https://sf.snu.ac.kr/jeehoon.kang/phd-thesis}.

In summary, we propose the formal semantics of integer-pointer casts and relaxed-memory concurrency,
which are the defining characteristics of C and yet are omitted from the prior work because of their
inherent complexity.  Based on our work, revisions to the ISO C standard and mainstream compilers
such as GCC and LLVM are being prepared.  Furthermore, we generalize CompCert to support for
separate compilation use cases---the majority of the real-world use cases for embedded
software---with a low verification cost.  As a result, we close the gap between the theory and
practice of systems programming in several dimensions.  As future work, by further pursuing the
research agenda for the formalization of real-world practice of C semantics and compilers, we aim to
provide clear, theoretically-informed, and practically-relevant C semantics and compilers that
system programmers can immediately benefit from.  If successful, our formal semantics will be able
to replace the C dialects used in Linux and other systems.

% by formalizing the semantics of other complex low-level features---such as type-based alias
% analysis, union, inline assembly, GPUs and accelerators---and by verifying mainstream compilers.


% TODO: how to mention these papers?
%
% \begin{itemize}
% \item[\cite{scfix}] Ori Lahav, Viktor Vafeiadis, \textbf{Jeehoon Kang}, Chung-Kil Hur, Derek Dreyer.
%   \emph{Repairing Sequential Consistency in C/C++11}.  \textbf{PLDI 2017}.
% \item[\cite{crellvm}] \textbf{Jeehoon Kang}*, Yoonseung Kim*, Youngju Song*, Juneyoung Lee, Sanghoon
%   Park, Mark Dongyeon Shin, Yonghyun Kim, Sungkeun Cho, Joonwon Choi, Chung-Kil Hur, Kwangkeun Yi.
%   (*The first three authors contributed equally and are listed alphabetically.)  \emph{Crellvm:
%     Verified Credible Compilation for LLVM}.  \textbf{PLDI 2018}.
% \end{itemize}


\paragraph{Organization}

\Cref{sec:background} provides the technical background on CompCert that informs the rest of this
dissertation.  \Cref{chap:intptrcast} presents a formal semantics of integer-pointer casts,
\Cref{chap:promising} presents a formal semantics of relaxed-memory concurrency, and
\Cref{chap:sepcomp} presents lightweight verification techniques for separate compilation.  This
dissertation concludes with \Cref{chap:epilogue}, which summarizes the contributions and the
impacts of this dissertation (\Cref{sec:conclusion}) and raises future research questions
(\Cref{sec:futurework}).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
