\section{Introduction}
\label{sec:introduction}

\subsection{The C Programming Language: the Compiler/Software Interface}
\label{sec:introduction:context}

\paragraph{Hardware Abstraction}

The C programming language is used as the \emph{lingua franca} for systems programming, mainly due
to its two notable advantages: \emph{portability} and \emph{control} over hardware.  C is portable
in the sense that C programs can be compiled and then executed in most of the existing hardware.  At
the same time, C has a precise control over hardware in the sense that C allows programmers to
access low-level hardware features such as memory layout and concurrency.  These advantages for
decades have attracted system programmers, who have built a giant ecosystem around the C language
itself and tools such as optimizing compilers, linkers, and program analyzers.

C enjoys portability and control---seemingly conflicting properties---at the same time thanks to the
fact that it is a balanced abstraction over various assembly languages, which are the interface of
hardware to software developers.  If C were exposing too much detail of hardware, then it would have
not been able to support some assembly languages that mismatch with the exposed details, losing a
significant degree of portability; on the other hand, if C were exposing too little detail of
hardware, then it would have lost the precise control over them.  The design choice of C as a
hardware abstraction is so popular that other systems programming languages---such as C++, D,
Objective C, Swift, and Rust---largely follow the design of C and are often called ``C-like''.

% To summarize, the C programming language is an abstraction that should satisfy the desiderata for
% three different ``masters'': portability for programmers, control for hardware, and optimization for
% compilers.

% C---like all the other programming languages---serves multiple ``masters'', namely programmers,
% compilers, and hardware.  From programmer's point of view, C should support \emph{reasoning
%   principles} that are powerful enough to reason about real-world C programs and guarantee their
% safety and functional correctness.  On the other hand, C should validate compiler and hardware
% \emph{optimizations} that may vastly accelerate the execution of C programs and are therefore
% actually performed in the real-world compilers and hardware.  What is particularly interesting about
% C is that ...


\paragraph{Compiler Optimization}

However, C is not just a thin wrapper around assembly languages because of compiler optimizations.
They have been so crucial for the performance of systems since the early days that every system
programmer expect a compiler to perform, \eg{} register promotion~\cite{mem2reg} and register
allocation~\cite{regalloc}, which are very effective and yet quite sophisticated compiler
optimizations.  Optimizations are becoming more and more important these days because recent
hardware trends---such as SIMD, GPU, and accelerators---offer potential for compiler optimizations
to further improve the performance of systems.  Since optimizations are an essential ingredient of
the real-world practice of C programming, the language should be an abstraction not only over just
hardware assembly languages but also over compiler optimizations.

% system programmers are building bigger systems, which they cannot hand-optimize on their own; and
% hardware vendors are introducing complex features, which need a special attention for maximal
% utilization.

% These days the mainstream compilers are becoming so aggressive in these days that they are
% performing even subtle optimizations that cannot be immediately justified.

Designing an abstraction over both hardware assembly languages and compiler optimizations, however,
is a challenging task.  For instance, consider the following transformation that is actually
performed by the mainstream compilers such as GCC~\cite{gcc} and LLVM~\cite{llvm}:

\[\begin{array}{rcl}
\begin{minipage}{0.27\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(a);
}
\end{minted}
\end{minipage}
&
\optarrow
&
\begin{minipage}{0.4\textwidth}
\begin{minted}{c}
void f() {
1: int a = 42;
2: g();
3: print(42); // const. prop.
}
\end{minted}
\end{minipage}
\end{array}\]

\noindent Suppose \code{g()} is an external function whose body is unknown to the compiler, and
\code{print(a)} prints the value of \code{a} to the screen.  The function \code{f()} first assigns
\code{42} to the local variable \code{a} (line \code{1}), calls some unknown external function
\code{g()} (line \code{2}), and then prints \code{a} (line \code{3}).  As an optimization,
mainstream compilers replace \code{a} with \code{42} at line \code{3}, effectively propagating the
constant \code{42} at line \code{1} to line \code{3}.  Such a \emph{constant propagation}
optimization is valid even in the presence of a function call to the unknown function \code{g()}, at
least in the viewpoint of compilers, because the address of the variable \code{a} is not leaked to
\code{g()} and thus \code{g()} cannot modify the content of \code{a}.

But what if \code{g()} is adversarially trying to \emph{guess} the address of \code{a} as follows?
%
\[
\begin{minipage}{0.8\textwidth}
\begin{minted}{c}
void g() {
1: int anchor;
2: int *guess = &anchor + 10; // guessing &x
3: *guess = 666;
}
\end{minted}
\end{minipage}
\]
%
\noindent The function \code{g()} tries to guess the address of \code{a} by exploiting the fact that
stack usually grows downwards with a fixed offset: it first declares a variable \code{anchor} and
guesses that \code{a} is located 10 words later than than \code{anchor} is.  Alas, the guess happens
to be correct when the adversarial \code{g()} is separately compiled with GCC 8.2.1 and then linked
with the original \code{f()}, and in this case, \code{f()} will surprisingly print the evil value
666.  On the other hand, when \code{g()} is linked with the optimized \code{f()}, \code{f()} will
print the expected value 42.\footnote{We got these results with GCC 8.2.1 and compile option
  \code{-fno-stack-protector} in Linux 4.18.} This example invalidates the compiler's analysis that
\code{a} is accessible only within the function \code{f()}, putting the soundness of the
optimization in danger.


\paragraph{Undefined Behavior}

In order to rescue the soundness of constant propagation, C blames the adversarial \code{g()} by
marking it as invoking \emph{undefined behavior}~\cite[\S3.4.3p1]{c18}: \code{g()} is not following
the rule of C so that compilers can do anything it chooses, \eg{} even ``to make demons fly out of
your nose''~\cite{nasal-demons}.  Specifically, \code{g()} invokes undefined behavior in the ISO C18
standard~\cite{c18} because the pointer \code{guess} is derived from the memory allocation of
\code{anchor} and yet is outside of the valid range of \code{anchor}, rendering itself invalid as an
address~\cite[\S6.5.6p8]{c18}.  In other words, compilers may safely assume that all the pointers
derived from \code{anchor} \emph{shall} point to \code{anchor}, and a failure to conform with such
an assumption gives compilers the right to generate arbitrary binary code.

It is worth noting that, in the course of establishing the soundness of constant propagation, the
ISO C18 standard is intentionally deviating from the hardware assembly language, where pointers have
the same representation as integer values of the appropriate width, and they simply index into a
single flat array representing memory.\footnote{This ``concrete'', flat memory model is supported
  via virtual page mapping in the operating systems.}

% The constant propagation example above shows that the low-level control over memory layout via
% pointer manipulation conflicts with a simple compiler optimization, and C resolves the conflict by
% marking the accesses to the outside of a memory allocation's valid range as invoking undefined
% behavior.

This kind of conflicts and resolutions (using undefined behavior) are common in the design of the C
programming language semantics.  For example, the example above shows that low-level control over
memory layout via pointer manipulation in C conflicts with a simple compiler optimization, and the
conflict is resolved by marking the accesses to the outside of a memory allocation's valid range as
invoking undefined behavior.  More generally, the C language has many conflicts between control over
various low-level details of hardware and various compiler optimizations implemented in mainstream C
compilers, and these conflicts are usually resolved by marking some unintended usage of language
features as invoking undefined behavior.

C is distinct in that undefined behaviors are not necessary for higher-level languages---such as
Java, C\#, OCaml, Haskell---to justify compiler optimizations thanks to their lack of precise
control over low-level details of hardware.  For example, constant propagation is immediately
justified in Haskell without resorting to undefined behavior thanks to its lack of raw pointer.

% Indeed, C has a lot of undefined behavior instances in the language semantics.  The C language is
% particularly interesting in that..  the widespread use of undefined behavior for resolving



\subsection{Compiler Optimization as a Threat to Compiler Reliability}
\label{sec:introduction:problem}

Unfortunately, undefined behavior is used to justify \emph{controversial} optimizations, which are
being more and more adopted in sophisticated mainstream compilers.  In order to improve performance
and energy consumption of systems, compiler writers have introduced dozens of subtle optimizations
even though their soundness is justified solely by intuition.  Those optimizations are subtle in
that they often introduce surprising non-local changes in program behavior and difficult-to-find
bugs~\cite{wang2013towards,yang2011finding}.  For example, mainstream compilers reason backwards
from instances of undefined behavior to conclude that, \eg{} certain code paths must be dead and to
remove the code.  To justify those subtle optimizations only afterwards, the ISO C standard marks
certain programs as invoking undefined behavior with a variety of ad-hoc exceptions, making the
already informal semantics written in English more ambiguous and confusing.  The ambiguity and
confusion render the soundness of various compiler optimizations controversial.

% As a consequence, the standard is not widely accepted in the systems programming community, \eg{}
% the Linux community defines its own dialect of C that supports much less compiler optimizations and
% is closer to assembly languages than the standard.  

% ---C89, C99, C11, and C18---

% for example, certain code paths must be dead and to remove those paths.  to conclude that,

% In order to mitigate the problem caused by reckless development of C semantics and compilers, ISO
% have revised the C semantics in a series of standards---C89, C99, C11, and C18---but they are still
% complex and 

% The ISO C18 standard~\cite{c18} famously does not give semantics to a significant subset of
% syntactically valid C programs. Instead, many programs exhibit implementation-defined, unspecified,
% or undefined behavior, with the latter imposing no requirements on a conforming implementation.

Those controversial optimizations cause the following problems:
%
\begin{itemize}
\item Those optimizations are bluring what is right and wrong for C compilers, thereby severely
  threatening the reliability of compilers.  Firstly, compilers are performing unpredictable
  optimizations so that the compiled program may not behave as expected by programmers.  Even worse,
  some optimizations make different, conflicting assumptions on the underlying semantics and
  collectively result in miscompilation bugs.  Those bugs are difficult to fix because it is unclear
  which optimizations are to blame: in particular, those bugs are not effectively found and fix by
  testing due to unclear specification.

\item As a consequence, mainstream C compilers are typically unused for safety-critical systems, and
  even when it is used, programmers turn off most compiler optimizations.  This practice
  significantly increases verification cost and degrades performance of safety-critical systems.
\end{itemize}

% because $(1)$ miscompilation bugs are hard to find because they typically manifest themselves very
% rarely, and $(2)$ they are hard to fix due to unclear specification.

% even experienced system programmers disagree on the semantics of numerous language features 


\paragraph{A Digression: a Miscompilation Bug}

\begin{figure}[t]
\begin{center}
\small
\begin{minipage}{0.5\textwidth}
\begin{minted}[bgcolor=white]{c}
void main() { 
  int x = 0;
  uintptr_t xi = (uintptr_t) &x;
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=gray]{c}
  uintptr_t i;
  for (i = 0; i < xi; ++i) {}
  if (xi != i) {
    printf("unreachable\n");
    xi = i;
  }
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=white]{c}
  auto p = (int *) xi;
  *p = 1;
  printf("%d\n", x); // expected: 1, actual: 0
}
\end{minted}
\end{minipage}
\end{center}
\caption{A GCC bug in the presence of integer-pointer casts}
\label{fig:introduction:bug}
\end{figure}

\Cref{fig:introduction:bug} presents a GCC miscompilation bug we found due to conflicting
optimizations.\footnote{We reported this miscompilation bug as a comment in
  \url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=65752}, which is still open as of this writing.}
(At first, ignore the gray area.)  Note that the type \code{uintptr\_t} is an integer type that is
able to hold a pointer value~\cite[\S7.20.1.4]{c18}.  In the program, the pointer to the local
variable \code{x} is cast to an integer, \code{xi}, and then cast back to pointer, \code{p}.  Thus
\code{p} points to \code{x}, and after \code{*p} is assigned one, the value of \code{x} should be
one.  Now inserting the code in the gray area should not change the program's behavior, because the
gray area is dead code: after the for loop, \code{i} equals to \code{xi}, and the conditional branch
is not taken.  But GCC miscompiles the program as follows:
%
\begin{enumerate}
\item Code motion optimization moves \mintinline{c}{xi = i} out of the conditional branch, because
  regardless of whether the branch is taken, \code{xi = i} should hold.
\item Alias analysis thinks that \code{p} is not a valid pointer, because it originates from
  \code{xi}, which equals to \code{i}, which in turn is obtained by just incrementing by one several
  times from zero.
\item Constant propagation optimization replaces \code{x} at the last line with zero, because
  \code{x} is initialized with zero and no intervening stores are writing to \code{x}.  In
  particular, \code{p} is not aliased with \code{\&x}, at least from the compiler's point of view,
  because \code{p} is not a valid pointer.
\end{enumerate}

\noindent It is unclear which optimizations or analyses involved with this bug are to blame, because
all of them look fine: the code motion and constant propagation optimizations seem legit, and the
alias analysis has a reasonable cause to believe that \code{p} is invalid.  But still, they are
conflicting with each other and they collectively result in a miscompilation bug.



\subsection{Formal Semantics and Compiler Verification}

Recall that controversial optimizations cause two problems: $(1)$ C compilers have unclear
specification, and $(2)$ they are typically not used for safety-critical systems.  To address these
problems, researchers have proposed to use the following methods:

\begin{itemize}
\item \textbf{Desinging formal semantics}~\cite{norrish1998c,leroy:compcert,ellison2012executable}:
  formal semantics is a mathematically clear definition of program behaviors, as opposed to informal
  English prose as in the ISO C18 standard.  Thanks to mathematically clear definition, it
  completely removes the ambiguity and confusion in the semantics.

\item \textbf{Verifying compilers}~\cite{compcert,vellvm}: verified compilers are accompanied with
  \emph{machine-checked} correctness proof, which formally guarantees that copmilers preserve the
  semantics of source programs and do not introduce any bugs.  Thanks to the absence of such
  miscompilation bugs, they can be used for safety-critical systems with confidence.
\end{itemize}

% Accordingly, there have been numerous efforts to capture the subtleties of the C standard
% formally, either by giving an alternative language definition or a conforming implementation.

\paragraph{Prior Art}

A landmark in this research direction is the CompCert C compiler~\cite{compcert}, which was
initiated by Xavier Leroy over ten years ago and grows as the first realistic verified compiler.
The CompCert compiler is realistic in the sense that it ``could realistically be used in the context
of production of critical software''.  In particular, it compiles a significant subset of ISO C99
down to assembly, and it performs a number of common and useful optimizations.  It is verified in
the sense that it ``is accompanied by a machine-checked proof [in Coq] of a semantic preservation
property: the generated machine code behaves as prescribed by the semantics of the source program.''
As such, CompCert guarantees that program analyses and verifications performed on its input carry
over soundly to its machine-level output.  It has served as a fundamental building block in academic
work on end-to-end verified software~\cite{TODO}, as well as receiving significant interest from the
avionics industry~\cite{TODO}.  Recently, it is certified for being used for nuclear power
plant~\cite{compcert-nuclear}.

% Over ten years ago, Xavier Leroy initiated the CompCert
% project~\cite{compcert:CACM,leroy:compcert}, a landmark effort that
% resulted in the first \emph{realistic verified compiler}.  The
% CompCert compiler~\cite{compcert-website} is \emph{realistic} in the
% sense that it ``could realistically be used in the context of
% production of critical software''.  In particular, it compiles a
% significant subset of the C language down to assembly, and it performs
% a number of common and useful optimizations.  It is \emph{verified} in
% the sense that it ``is accompanied by a machine-checked proof [in Coq]
% of a semantic preservation property: the generated machine code
% behaves as prescribed by the semantics of the source program.''  As
% such, CompCert guarantees that program analyses and verifications
% performed on its input carry over soundly to its machine-level output.
% It has served as a fundamental building block in academic work on
% end-to-end verified software~\cite{appel:plcc}, as well as receiving
% significant interest from the avionics industry~\cite{avionics}.
% % http://projects.laas.fr/IFSE/FMF/J3/slides/P05_Jean_Souyiris.pdf
% % XXX: Any other projects that depend on CompCert besides Appel's
% % XXX: Verified Software Toolchain or Airbus?

In the same spirit as CompCert, Vellvm~\cite{vellvm} by Steve Zdancewic and his collaborators
formalizes a significant subset of the LLVM IR (intermediate representation) and verifies
interesting compiler transformations and optimizations performed at the IR level.  Most notably,
Vellvm formalizes the static single assignment form (SSA), including a dominance analysis, an SSA
type checker, and an SSA-aware register promotion algorithm that is simplified from the
\code{mem2reg} pass in LLVM.

% Because of these simplifications, they are currently suitable only for a niche market such as
% safety-critical embedded systems.

However, these projects on formal semantics and compiler verification makes unrealistic simplifying
assumptions on C semantics and compilers, skating over the complexity of the real-world practice of
systems programming.  While CompCert and Vellvm support a significant subset of C99 and LLVM IR,
respectively, they lack support for various low-level features that are crucially used in many
system programs, such as memory layout, concurrency, and processor register manipulation.
Furthermore, CompCert and Vellvm only perform quite straightforward transformations and
optimizations that are way less sophisticated than those in mainstream compilers, and support only
limited use cases of compilers, \eg{} they did not verify linking.


\subsection{Embracing the Real-World Practice of C Programming}

In this dissertation, we carry forward the research direction for formal semantics and compiler
verification for C by accounting for more features and use cases that are crucial in the real world
and yet were beyond the reach of the prior work.  First, we propose a formal semantics of
integer-pointer casts (\Cref{chap:intptrcast}) and relaxed-memory concurrency (\Cref{chap:relaxed}).
These features are crucial in that they allow cross-platform low-level access to memory and are
essential for applications such as operating system kernels and language runtimes.  Second, we
generalize CompCert to support for separate compilation with low verification cost
(\Cref{chap:sepcomp}).  Separate compilation is crucial in that it significantly reduces compilation
time.  To establish confidence in our contributions, we have formalized most of the key results in
Coq.  The formalization is available online~\cite{kang-phd-thesis-web}.


Now we will explain the contribution of each chapter in more details.

% However, it is difficult to design a formal semantics for these features that justifies
% optimizations for compilers and supports reasoning principles for programmers because of their
% inherent complexity.  due to subtle pointer aliasing rules~\cite{krebbers2013aliasing}, reliance
% on implementation-specific behavior, the treatment of pointers to uninitialized memory, and
% hardware/compiler optimizations.
% We propose the first such formal semantics that satisfies both compilers and programmers at the same
% time.  Based on our work, revisions to the ISO C standard and mainstream compilers such as GCC and
% LLVM are being prepared.



\paragraph{\Cref{chap:intptrcast}: Cast between Integers and Pointers}

While cast between integers and pointers is one of the defining characteristics of the C programming
language, the feature has not been formalized in CompCert and Vellvm because it drastically
conflicts with major compiler optimizations.  The ISO C standards try to reconcile the feature and
the optimizations using the notion of \emph{provenance}, but it fails to support certain common
optimizations and requires an intrusive change to the language semantics.

In this chapter, we propose the first formal semantics of casts between integers and pointers that
$(1)$ fully supports operations on the representation of pointers, including all arithmetic
operations for pointers that have been cast to integers, $(2)$ validates major compiler
optimizations on memory accesses, and $(3)$ is simple to understand and program with.  The key novel
idea behind our semantics is the notion of \emph{concretization}: when allocated, a memory block is
not assigned a concrete address yet; only when it is required by a pointer-to-integer cast, the
block is lazily assigned a concrete address, \ie{} the block is concretized.

In the course of doing this research, we discovered the GCC bug presented in
\Cref{fig:introduction:bug}, with which we persuaded compiler writers that it is safe to turn off
some too aggressive alias analyses.  Furthermore, our idea has subsequently been refined by
follow-up papers by other researchers~\cite{intptrcast-oopsla,intptrcast-popl}, which are
accompanied with promising revision proposals to the LLVM compiler and ISO C standard.


% The ISO C standard does not specify the semantics of many valid programs that use non-portable
% idioms such as integer-pointer casts. Recent efforts at formal definitions and verified
% implementation of the C language inherit this feature.  By adopting high-level abstract memory
% models, they validate common optimizations. On the other hand, this prevents reasoning about much
% low-level code relying on the behavior of common implementations, where formal verification has
% many applications.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{intptrcast} \textbf{Jeehoon Kang}, Chung-Kil Hur, William Mansky, Dmitri Garbuzov,
  Steve Zdancewic, Viktor Vafeiadis.  \emph{A Formal C Memory Model Supporting Integer-Pointer
    Casts}.  \textbf{PLDI 2015}.}

\paragraph{\Cref{chap:relaxed}: Relaxed-Memory Concurrency}

Despite many years of research, it has proven very difficult to develop a formal semantics for
concurrent programming languages that adequately balances the conflicting desiderata of programmers,
compilers, and hardware.

In this chapter, we propose the first formal semantics of relaxed-memory concurrency that $(1)$
accounts for a broad spectrum of low-level concurrency features in C, $(2)$ is implementable, in the
sense that it provably validates many standard compiler optimizations and reorderings, as well as
standard compilation schemes to x86-TSO, $(3)$ justifies simple invariant-based reasoning, thus
demonstrating the absence of bad ``out-of-thin-air'' behaviors, $(4)$ supports ``DRF'' guarantees,
ensuring that programmers who use sufficient synchronization need not understand the full
complexities of relaxed-memory semantics, and $(5)$ defines the semantics of racy programs without
relying on undefined behaviors, which is a prerequisite for applicability to type-safe languages
like Java.  The key novel idea behind our semantics is the notion of promises: a thread may promise
to execute a write in the future, thus enabling other threads to read from that write out of order.
Crucially, to prevent out-of-thin-air behaviors, a promise step requires a thread-local
certification that it will be possible to execute the promised write even in the absence of the
promise.

\jeehoon{redo it.}  Our semantics is a promising proposal for relaxed-memory concurrency semantics
for C/C++ and compiler IRs, and draws interest from both industry and academia.  In particular, our
semantics serves as a guide to C/C++ relaxed-memory concurrency, and reasoning principles for our
semantics are being developed.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{promising} \textbf{Jeehoon Kang}, Chung-Kil Hur, Ori Lahav, Viktor
  Vafeiadis, Derek Dreyer.  \emph{A Promising Semantics for Relaxed-Memory Concurrency}.
  \textbf{POPL 2017}.}


\paragraph{\Cref{chap:sepcomp}: Lightweight Verification of Separate Compilation}

Major compiler verification efforts, such as CompCert and Vellvm, have traditionally simplified the
verification problem by restricting attention to the correctness of whole-program compilation,
leaving open the question of how to verify the correctness of separate compilation.  Recently, a
number of sophisticated techniques have been proposed for proving more flexible, compositional
notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the
simple ``closed simulations'' used in verifying whole-program compilation.  Applying such techniques
to a compiler like CompCert, as Stewart \etal{} have done, involves major changes and extensions to
its original verification.

In this chapter, we show that if we aim somewhat lower---to prove correctness of separate
compilation, but only for a single compiler---we can drastically simplify the proof effort.  Toward
this end, we develop several lightweight techniques that recast the compositional verification
problem in terms of whole-program compilation, thereby enabling us to largely reuse the
closed-simulation proofs from existing compiler verifications.  We demonstrate the effectiveness of
these techniques by applying them to CompCert 2.4, converting its verification of whole-program
compilation into a verification of separate compilation in less than two person-months.  This
conversion only required a small number of changes to the original proofs.

We uncovered two compiler bugs along the way---one of which is on separate compilation and the other
is orthogonal to separate compilation---and our verification techniques are subsequently adopted in
CompCert 2.7.

This chapter draws heavily on the work and writing in the following paper:

\noindent {\small \cite{sepcomp} \textbf{Jeehoon Kang}, Yoonseung Kim, Chung-Kil Hur, Derek Dreyer,
  Viktor Vafeiadis.  \emph{Lightweight Verification of Separate Compilation}.  \textbf{POPL 2016}.}

\paragraph{Organization}

The rest of dissertation is organized as follows.  \Cref{sec:background} provides the technical
background on CompCert that informs the rest of this dissertation.
\Cref{chap:intptrcast,chap:relaxed,chap:sepcomp} present the main contributions of this
dissertation.  This dissertation concludes with \Cref{chap:epilogue}, which summarizes the main
contributions, the impacts, and future research directions (\Cref{sec:conclusion}).

% Specifically, we propose the formal semantics of several low-level features that are the defining
% characteristics of C and yet are omitted from the prior work.  Furthermore, we propose
% verification techniques for real-world compiler use cases.


% by formalizing the semantics of other complex low-level features---such as type-based alias
% analysis, union, inline assembly, GPUs and accelerators---and by verifying mainstream compilers.


% Other papers:
%
% \begin{itemize}
% \item[\cite{scfix}] Ori Lahav, Viktor Vafeiadis, \textbf{Jeehoon Kang}, Chung-Kil Hur, Derek Dreyer.
%   \emph{Repairing Sequential Consistency in C/C++11}.  \textbf{PLDI 2017}.
% \item[\cite{crellvm}] \textbf{Jeehoon Kang}*, Yoonseung Kim*, Youngju Song*, Juneyoung Lee, Sanghoon
%   Park, Mark Dongyeon Shin, Yonghyun Kim, Sungkeun Cho, Joonwon Choi, Chung-Kil Hur, Kwangkeun Yi.
%   (*The first three authors contributed equally and are listed alphabetically.)  \emph{Crellvm:
%     Verified Credible Compilation for LLVM}.  \textbf{PLDI 2018}.
% \end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
