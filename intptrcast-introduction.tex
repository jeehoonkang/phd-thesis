\section{Introduction}
% dmitri: Is this a taugology? Does any kind of programming _not_ rely on
% some kind of understanding of semantics? I think the intention here is to
% emphasize the difficulty of writing correct programs in C. Also, the problem
% addressed by the paper, arbitrary manipulation of pointers as integer values,
% is not identified early enough.
%
% C programmers rely crucially on a mental model of their program's
% expected behavior:  they use their understanding of C semantics to
% guide program development, to help with debugging, and to
% predict the effects of program transformations and optimizations made
% by the compiler.

\emph{Unrestricted} manipulation of the representation of pointers via cast between pointers and
integers is crucially used in low-level code.  For example, the Linux kernel and the HotSpot Java
virtual machine perform bitwise operations on pointers in maintaining page tables and live objects,
respectively.  For another example, the C++ standard library's hash function (\code{std::hash}) uses
the pointer's bit representation as a key, \eg{} for indexing into a hash table.  This is useful
since taking a pointer is a cheap way to get a unique key.

%As a consequence, most programmers probably rely on a fairly concrete
%model of C semantics in which pointer values are memory addresses,
%which are themselves ``just'' integers.  Together with some
%rules-of-thumb about data layout and the potential side effects of
%function calls, this model serves as a basis for reasoning about C
%programs.

% The extent to which the programmer's model of the C semantics agrees
% with that of the compiler implementor's is therefore an important
% aspect of C program development---incompatibility between the two
% perspectives can lead to hard-to-diagnose bugs or subtle dependencies
% on compiler implementation details.
%
% Unfortunately, the C11 standard, while it strives for precision and
% comprehensive coverage, is rather large, somewhat opaque, and
% relatively informal.  A significant amount of complexity in the
% standard is introduced to handle obscure cases or to justify the
% correctness of desirable compiler optimizations.


% The semantics in users' mind and that in compiler developer's must
% coincide. Otherwise, compiled program may behave in an unexpected way
% in a user's point of view. For example, compiler version up may make
% previously well behaved programs to crash.  Also, compiler verification
% result can be weakened.

%% \todo{Dead allocation elimination is another good example. It might be
%%   good to at least briefly explain the example.}


% dmitri: Would it be more accurate to say that the concrete memory model is the
% _simplest_ memory model that can support the arbitrary pointer manipulation
% that we're after rather than that all C memory models fall into these two categories?
% I'm also hesitant to make claims about what C programmers expect of
% the memory model.

\paragraph{Problem}

% One popular extension of the standard C memory model that 

However, none of the existing proposals for C/C++ semantics fully supports manipulation of pointers
as integer values \emph{and} compiler optimizations at the same time.  ISO C18 standard provides an
integer type $\mathtt{uintptr\_t}$ that may be legally cast to and from pointer types, it does not
require anything of the resulting values~\cite[\S7.20.1.4p1]{c18}.  The concrete model
straightforwardly supports bit-level pointer manipulation, but as we have seen in
\Cref{sec:introduction,sec:background:memory}, this model invalidates many basic compiler
optimizations such as constant propagation and dead allocation elimination.  On the other hand,
CompCert's logical model presented in \Cref{sec:introduction,sec:background:memory} supports such
compiler optimizations, but it does not support cast between pointers and integers---as well as many
other low-level features in C---because the logical model represents pointers as pairs of an
allocation block identifier and an offset within that block, which cannot be easily casted into and
from an integer.

% As a consequence, logical memory models treat programs containing those idioms as undefined (\ie
% erroneous).

% In order to enable such compiler optimizations, most work on formal semantic sand compiler
% verification instead relies on logical memory models,

% due to the combination of finite memory and casts of arbitrary integers to pointers.

Designing a satisfactory semantics of integer-pointer casts is challenging.  For example, such a
semantics should point out which optimization(s)---each of which seems legit at first glance---is to
blame in the LLVM miscompilation bug presented in \Cref{fig:introduction:bug}.

\begin{figure}[t]
\begin{center}
\small
\begin{minipage}{0.5\textwidth}
\begin{minted}[bgcolor=white]{c}
void main() { 
  int x = 0;
  uintptr_t xi = (uintptr_t) &x;
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=gray]{c}
  uintptr_t i;
  for (i = 0; i < xi; ++i) {}
  if (xi != i) {
    printf("unreachable\n");
    xi = i;
  }
\end{minted}
\vskip -0.7cm
\begin{minted}[bgcolor=white]{c}
  auto p = (int *) xi;
  *p = 1;
  printf("%d\n", x); // expected: 1, actual: 0
}
\end{minted}
\end{minipage}
\end{center}
\caption{A GCC bug in the presence of integer-pointer casts}
\label{fig:intptrcast-introduction:bug}
\end{figure}

Such a semantics should also guide compiler writers how to fix the GCC miscompilation bug we found,
which is presented in \Cref{fig:intptrcast-introduction:bug}.\footnote{We reported this
  miscompilation bug as a comment in \url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=65752}, which
  is still open as of this writing.}  (At first, ignore the gray area.)  In the program, the pointer
to the local variable \code{x} is cast to an integer, \code{xi}, and then cast back to pointer,
\code{p}.  Thus \code{p} points to \code{x}, and after \code{*p} is assigned one, the value of
\code{x} should be one.  Now inserting the code in the gray area should not change the program's
behavior, because the gray area is dead code: after the for loop, \code{i} equals to \code{xi}, and
the conditional branch is not taken.  But GCC miscompiles the program as follows:
%
\begin{enumerate}
\item Code motion optimization moves \mintinline{c}{xi = i} out of the conditional branch, because
  regardless of whether the branch is taken, \code{xi = i} should hold.
\item Alias analysis thinks that \code{p} is not a valid pointer, because it originates from
  \code{xi}, which equals to \code{i}, which in turn is obtained by just incrementing by one several
  times from zero.
\item Constant propagation optimization replaces \code{x} at the last line with zero, because
  \code{x} is initialized with zero and no intervening stores are writing to \code{x}.  In
  particular, \code{p} is not aliased with \code{\&x}, at least from the compiler's point of view,
  because \code{p} is not a valid pointer.
\end{enumerate}
%
% \noindent It is unclear which optimizations or analyses involved with this bug are to blame, because
% all of them look legit: the code motion and constant propagation optimizations seem legit, and the
% alias analysis has a reasonable cause to believe that \code{p} is invalid.  But still, they are
% conflicting with each other and they collectively result in a miscompilation bug.

%\todo{Show example with pointer to integer cast.}
%Gil:
%{%% \small
%\begin{lstlisting}
%void set_attribute (struct hash * h,
%     struct tree * v, struct attribute * w)
%{
%  hash_put(h, (void*) v, (void*) w)
%}
%\end{lstlisting}}


\paragraph{Our Solution}

In this chapter, we propose a C/C++ memory model that gives semantics to programs that manipulate
the bit-level representation of pointers, and yet permits the same optimizations as logical models
for code not using these low-level features.  The key technical ingredient for making this work is
combining the strengths of the concrete and logical memory models: now pointer values have two
distinct representations, a concrete and a logical one, and they can be converted to each other.  By
default, a pointer is represented logically, and only when it is cast to an integer type, is the
logical pointer value \emph{concretized} to a concrete 32-bit integer (or 64-bit integer depending
on the architecture).  When an integer is cast back to a pointer value, it is mapped to the
corresponding logical address.
%% it maintains its concrete
%% representation, and only mapped to the corresponding logical address
%% in memory accesses.

% \emph{hybrid memory model} for C/C++ that 

% , while retaining a model that is simple for the programmer to reason about.

With our hybrid model, we propose how to fix both the LLVM bug presented in
\Cref{fig:introduction:bug} and the GCC bug presented in \Cref{fig:intptrcast-introduction:bug},
which is by turning off some controversial, too aggressive alias analysis.  Furthermore, we
generalized the existing compiler verification techniques for the logical memory model to also
account for our hybrid model.  Our model also supports all the reasoning principles designed for the
logical model---\ie{} any sound reasoning about programs in the logical model also holds in our
model---because our hybrid model conservatively extends the logical model: it gives semantics to
strictly more programs than those supported by the logical model without changing their semantics.

% Thus, any sound reasoning about programs in the logical model also holds in the hybrid model, but
% the hybrid model also supports reasoning about pointer arithmetic as in the concrete model.

% we achieve this without substantially complicating the proof techniques required for a verified
% compiler while retaining a model that is simple for the programmer to reason about.


% of those programs that do have semantics under the logical model.


% Finally, the hybrid model is not intended to replace the
% memory model in the C standard. Like the concrete and logical models,
% it is a formal refinement of the (informal) ISO C standard that can be
% used for formally reasoning about programs and program transformations
% (as in compiler verification).

\medskip \noindent
To summarize, our contributions are:
\begin{itemize}
\item The first formal semantics that \emph{fully} supports unrestricted manipulation of pointer
  values via integer-pointer casts and yet allows the standard compiler optimizations
  (\Cref{sec:intptrcast:formal-semantics}).

\item A proposal to fix to the LLVM bug presented in \Cref{fig:introduction:bug} and the GCC bug
  presented in \Cref{fig:intptrcast-introduction:bug} (\Cref{sec:intptrcast:fix}).

\item Compiler verification techniques for proving semantic preservation under our semantics and
  their application to verify a number of standard optimizations in the presence of integer-pointer
  casts (\Cref{sec:intptrcast:compiler-verification}).
\end{itemize}

\noindent \Cref{sec:intptrcast:discussion} discusses the related work and the impact of our memory
model.  All the proofs reported in this chapter have been fully formalized in Coq and is available
online~\cite{kang-phd-thesis-web}.

%However, there is a big gap between the two.  The concrete semantics
%does not justify useful compiler optimizations.  So, the standard
%semantics, to justfiy such optimizations, have many informal, complex
%exceptional rules that users do not expect, and thus is hard to
%formalize. Compiler verification people use simple, formal logical
%semantics which justify optimizations.  However, the semantics does
%not support pointer-integer casting, a feature that users expect to
%use.
%
%
%The concrete semantics has a problem. use the following example to
%explain the lack of justifying optimizations.
%\begin{verbatim}
%void f(int i) {
%  int a[10];
%  int b = 0;
%  a[i] = 1;
%  return b;    -> return 0
%}
%\end{verbatim}
%
%The standard semantics solves the above problem using some
%exceptional rule. explain the rule and discuss the problems.
%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
