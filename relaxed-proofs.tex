\section{Proofs}
\label{sec:relaxed:proofs}

In this section, we present notable proofs of the results presented in \Cref{sec:results}.  We first
present our thread-local simulation relation for proving the soundness of transformations
(\Cref{sec:relaxed:proofs:simulation}).  Then we present the proofs of DRF-RA
(\Cref{sec:relaxed:proofs:drfra}) and DRF-LOCK (\Cref{sec:relaxed:proofs:drflock}), which are the
only theorems not formalized in Coq.


\subsection{Thread-Local Simulation Relation}
\label{sec:relaxed:proofs:simulation}

As discussed in \Cref{sec:transformations}, our thread-local simulation relation is based on novel
ideas for reasoning about the shared resources and consistency.  Now we discuss each point in more
details and present the simulation's definition.


\paragraph{Machine Simulation Relation}

But before delving into the details, we first review simulation relations.  Recall from
\Cref{sec:background:correctness} that a relation $\mathcal{R}$ over source and target machine
states is a (machine) simulation if, for any source and target states $\mconf_0$ and $\mconf'_0$
related by $\mathcal{R}$, the following holds:
\begin{description}
\item[\textsc{(Step)}] For all $\mconf'_1$, if $\mconf'_0 \astep{} \mconf'_1$, then there exists
  $\mconf_1$ such that $\mconf_0 \astep{}^* \mconf_1$ and $\mathcal{R}$ relates $\mconf_1$ and
  $\mconf'_1$; and
\item[\textsc{(Terminal)}] If $\mconf'_0$ is terminal, then there exists $\mconf_1$ such that
  $\mconf_0 \astep{}^* \mconf_1$ and $\mconf_1$ is terminal.
\end{description}
%
Here, we intentionally omitted events in the definition of simulation relation for presentational
purposes, but the discussion in this section applies also to simulation relations with events.  Also
recall that if two states are simulated, then the behavior of the target state is a subset of that
of the source state.


\paragraph{Reasoning about Shared Resources}

Now we are about to ``localize'' the definition of simulation relation to thread configurations.
Our goal is proving compositionality of thread-local simulation: if $\tup{\gts,\gsco,\mem}$ and
$\tup{\gts',\gsco',\mem'}$ are source and target machine states, and for each $i$, there exists a
thread-local simulation relation $R_i$ that relates $\tup{\gts(i),\gsco,\mem}$ and
$\tup{\gts'(i),\gsco',\mem'}$, then there exists a machine simulation relation $\mathcal{R}$ that
relates $\tup{\gts,\gsco,\mem}$ and $\tup{\gts',\gsco',\mem'}$.

A naive attempt to define thread-local simulation would be directly translating machine simulation
relation to thread configurations.  However, it does not satisfy compositionality because, in a
thread's point of view, the other threads may change the shared resources at any time.  For example,
for machine states $\tup{\gts_0,\gsco_0,\mem_0}$ and $\tup{\gts'_0,\gsco'_0,\mem'_0}$, if
$\gts_0(1)$ and $\gts'_0(1)$ take steps, the shared resources $\gsco_0$, $\mem_0$, $\gsco'_0$, and
$\mem'_0$ may be changed, say to $\gsco_1$, $\mem_1$, $\gsco'_1$, and $\mem'_1$, and thus
$\tup{\gts_0(2),\gsco_1,\mem_1}$ and $\tup{\gts'_0(2),\gsco'_1,\mem'_1}$ are no longer
thread-locally simulated.

Thus in the definition of thread-local simulation, we need to take into account the effect---or the
interference---of the other threads to the shared resources at any time.  Our key observation is
that the interference of the other threads can be abstractly characterized as follows.  Suppose
$\tup{\lts_0,\gsco_0,\mem_0}$ and $\tup{\lts'_0,\gsco'_0,\mem'_0}$ are the source and target thread
configurations in our concern, and the other threads changed the source and target shared resources
to $\tup{\gsco_1,\mem_1}$ and $\tup{\gsco'_1,\mem'_1}$.  Then the following holds:

\begin{description}
\item[\textsc{(Intrf-Future)}] $\tup{\gsco_1,\mem_1}$ is a future shared resource of
  $\tup{\gsco_0,\mem_0}$ w.r.t. $\lts_0.\lprmem$, or in other words, $\gsco_1 \sqsupseteq \gsco_0$
  and $\mem_1$ is a future memory of $\mem_0$ w.r.t. $\lts_0.\lprmem$.  Similarly,
  $\tup{\gsco'_1,\mem'_1}$ is a future shared resource of $\tup{\gsco'_0,\mem'_0}$
  w.r.t. $\lts'_0.\lprmem$;
\item[\textsc{(Intrf-SC)}] If $\gsco_0 \sqsubseteq \gsco'_0$, then $\gsco_1 \sqsubseteq \gsco'_1$;
  and
\item[\textsc{(Intrf-Memory)}] If $\mem_0 \sim \mem'_0$, then $\mem_1 \sim \mem'_1$.  Here, by
  $M \sim M'$ we mean $(1)$ for each location, the timestamps covered by the messages in $M$
  coincides with those covered by the messages in $M'$; and $(2)$ for each message
  $\msg{x}{v}{f'}{t}{\mrel'}$ in $M'$, there exists a message $\msg{x}{v}{f}{t}{\mrel}$ in $M$ such
  that $\mrel \sqsubseteq \mrel'$.
\end{description}
%
\noindent From now on, we write $\tup{\gsco,\mem} \sim \tup{\gsco',\mem'}$ to mean
$\gsco \sqsubseteq \gsco'$ and $\mem \sim \mem'$ for simplicity.

Now in proving thread-local simulations, we \emph{assume} that the interference of the other threads
are constrained by the above conditions; on the other hand, we also \emph{guarantee} that the thread
in our concern also satisfies the above conditions.  Concretely, we revise our thread-local
simulation relation as follows.  A relation $R$ over source and target thread configurations is a
thread-local simulation if:

\begin{align*}
  \forall \tup{\lts_0,\gsco_0,\mem_0}, \tup{\lts'_0,\gsco'_0,\mem'_0}.~ & R~\tup{\lts_0,\gsco_0,\mem_0}~\tup{\lts'_0,\gsco'_0,\mem'_0} \implies \\
  \forall \tup{\gsco_1,\mem_1}, \tup{\gsco'_1,\mem'_1}.~ & \tup{\gsco_0,\mem_0} \sim \tup{\gsco'_0,\mem'_0} \land \tup{\gsco_1,\mem_1} \sim \tup{\gsco'_1,\mem'_1} ~ \land \\
                                                                        & \mbox{$\tup{\gsco_1,\mem_1}$ is a future of $\tup{\gsco_0,\mem_0}$ w.r.t. $\lts_0$} ~ \land \\
                                                                        & \mbox{$\tup{\gsco'_1,\mem'_1}$ is a future of $\tup{\gsco'_0,\mem'_0}$ w.r.t. $\lts'_0$} \implies
\end{align*}
\begin{center}
  \begin{minipage}{0.9\textwidth}
    \begin{description}
    \item[\textsc{(Step)}] For any $\tup{\lts'_2,\gsco'_2,\mem'_2}$, if
      $\tup{\lts'_0,\gsco'_1,\mem'_1} \astep{} \tup{\lts'_2,\gsco'_2,\mem'_2}$, then there exists
      $\tup{\lts_2,\gsco_2,\mem_2}$ such that
      $\tup{\lts_0,\gsco_1,\mem_1} \astep{}^* \tup{\lts_2,\gsco_2,\mem_2}$, $R$ relates
      $\tup{\lts_2,\gsco_2,\mem_2}$ and $\tup{\lts'_2,\gsco'_2,\mem'_2}$, and
      $\tup{\gsco_2,\mem_2} \sim \tup{\gsco'_2,\mem'_2}$; and
    \item[\textsc{(Terminal)}] If $\tup{\lts'_0,\gsco'_1,\mem'_1}$ is terminal, then there exists
      $\tup{\lts_2,\gsco_2,\mem_2}$ such that
      $\tup{\lts_0,\gsco_1,\mem_1} \astep{}^* \tup{\lts_2,\gsco_2,\mem_2}$,
      $\tup{\lts_2,\gsco_2,\mem_2}$ is terminal, and
      $\tup{\gsco_2,\mem_2} \sim \tup{\gsco'_1,\mem'_1}$; and $\cdots$
    \end{description}
  \end{minipage}
\end{center}
%
Notice that it differs from the machine simulation relation, except for types, in that $(1)$ we
should prove simulation after all legit interference of the other threads (lines 2-4), and $(2)$ we
should prove the thread's modification to the shared resources is legit (lines 7 and 10).

It is worth noting that the above conditions on the shared resources resemble memory invariants in
(sequential) compiler verification literature, \eg{} memory extensions and injections in
CompCert~\cite{Leroy-Appel-Blazy-Stewart-memory-v2}.  Both are indeed introduced for the same
purpose, namely reasoning about the effect of unknown others, but they differ in what are the
``others'': it is other threads in our case, and it is other functions for \eg{} CompCert.




\paragraph{Reasoning about Consistency}

The above definition still does not satisfy compositionality because it fails to simulate
consistency.  Recall that for each machine step, the resulting thread configuration should be
consistent.  The above simulation relation does not necessarily mean the source thread configuration
steps to a consistent one even if the corresponding target thread configuration steps to a
consistent one.

To address this problem, we require the following conditions, in addition to the
\textbf{\textsc{(Step)}} and \textbf{\textsc{(Terminal)}} conditions above, for thread-local
simulation:
%
\begin{center}
  \begin{minipage}{0.9\textwidth}
    \begin{description}
    \item[\textsc{(Future)}] For any shared resource $\tup{\gsco_2,\mem_2}$ that is a future of
      $\tup{\gsco_1,\mem_1}$ w.r.t. $\lts_0.\lprmem$, there exists a shared resource
      $\tup{\gsco'_2,\mem'_2}$ that is a future of $\tup{\gsco'_1,\mem'_1}$ w.r.t. $\lts'_0.\lprmem$
      such that $R$ relates $\tup{\lts_0,\gsco_2,\mem_2}$ and $\tup{\lts'_0,\gsco'_2,\mem'_2}$; and
    \item[\textsc{(No promises)}] If $\lts'_0.\lprmem = \emptyset$, then there exists
      $\tup{\lts_2,\gsco_2,\mem_2}$ such that
      $\tup{\lts_0,\gsco_1,\mem_1} \astep{}^* \tup{\lts_2,\gsco_2,\mem_2}$ and
      $\lts_2.\lprmem = \emptyset$.
    \end{description}
  \end{minipage}
\end{center}
%
\noindent Provided that the above conditions hold, consistency is also simulated as follows.
Suppose that a thread-local simulation $R$ relates source and target thread configurations
$\tup{\lts_0,\gsco_0,\mem_0}$ and $\tup{\lts'_0,\gsco'_0,\mem'_0}$, and the target configuration is
consistent.  Now we prove $\tup{\lts_0,\gsco_0,\mem_0}$ is also consistent.  For any future shared
resources $\gsco_1,\mem_1$ of $\gsco_0,\mem_0$ w.r.t. $\lts_0.\lprmem$, by \textsc{(Future)}, there
exist $\gsco'_1,\mem'_1$ such that they are a future of $\gsco'_0,\mem'_0$ w.r.t. $\lts'_0.\lprmem$
and $R$ relates $\tup{\lts_0,\gsco_1,\mem_1}$ and $\tup{\lts'_0,\gsco'_1,\mem'_1}$.  Since
$\tup{\lts'_0,\gsco'_0,\mem'_0}$ is consistent, there exists $\tup{\lts'_2,\gsco'_2,\mem'_2}$ such
that $\tup{\lts'_0,\gsco'_1,\mem'_1} \astep{}^* \tup{\lts'_2,\gsco'_2,\mem'_2}$ and $\lts'_2$ has no
promises.  By \textsc{(Step)}, there exists $\tup{\lts_2,\gsco_2,\mem_2}$ such that
$\tup{\lts_0,\gsco_1,\mem_1} \astep{}^* \tup{\lts_2,\gsco_2,\mem_2}$ and $R$ relates
$\tup{\lts_2,\gsco_2,\mem_2}$ and $\tup{\lts'_2,\gsco'_2,\mem'_2}$.  By \textsc{(No promises)},
there exists $\tup{\lts_3,\gsco_3,\mem_3}$ such that
$\tup{\lts_2,\gsco_2,\mem_2} \astep{}^* \tup{\lts_3,\gsco_3,\mem_3}$ and $\lts_3$ has no promises.
Thus $\tup{\lts_0,\gsco_0,\mem_0}$ is consistent.

After taking into account the effect of the other threads to the shared resources and consistency,
we can prove thread-local simulation's compositionality by coinduction.



\subsection{Proof of DRF-RA}
\label{sec:relaxed:proofs:drfra}

\newcommand{\ME}{\textrm{ME}}
\newcommand{\AccMode}{\mathrm{AM}}
\newcommand{\mesilent}{\mathtt{silent}}
\newcommand{\meread}{\mathtt{read}}
\newcommand{\mewrite}{\mathtt{write}}
\newcommand{\meupdate}{\mathtt{update}}
\newcommand{\mefence}{\mathtt{fence}}
\newcommand{\mesyscall}{\mathtt{syscall}}
\newcommand{\mcths}{\mathtt{ths}}
\newcommand{\mcgsc}{\mathtt{gsc}}
\newcommand{\mcmem}{\mathtt{mem}}
\newcommand{\twomsg}[2]{\tup{#1\text{\small@}#2}}

\noindent
We first define the set of memory events $\alpha\in\ME$ as follows:
\[
\begin{array}{@{}c@{~}l@{}}
%% \alpha \in \ME 
 & \setof{\mesilent} \\
\cup& \setof{\meread(o,x,t) \suchthat o\in\AccMode ,x\in\Loc,t\in\Time } \\
\cup& \setof{\mewrite(o,x,t) \suchthat o\in\AccMode,x\in\Loc,t\in\Time} \\
\cup& \setof{\meupdate(o_\lr,o_\lw,x,t_\lr,t_\lw) \suchthat o_\lr,o_\lw\in\AccMode,x\in\Loc,t_\lr,t_\lw\in\Time } \\
\cup& \setof{\mefence(T) \suchthat T \in \setofz{\acqo,\relo,\sco} } \\
\cup& \setof{\mesyscall(v) \suchthat v \in ... } \\
\end{array}
\]
where $\AccMode = \setofz{\pln,\rlx,\ra}$. %\sco

We call the following events \emph{globally synchronizing}:
\[
\begin{array}{@{}c@{~}l@{}}
%\cup& \setof{\mewrite(o,x,t) \suchthat o = \sco} \\
%\cup& \setof{\meupdate(o_\lr,o_\lw,x,t_\lr,t_\lw) \suchthat o_\lw = \sco} \\
%\cup
& \setof{\mefence(\sco) } \\
\cup& \setof{\mesyscall(v) \suchthat v \in ... } \\
\end{array}
\]

Then we annotate promise-free and release-acquire steps with 
the executed thread ids and memory events, denoted $\bstep{}_{(i,\alpha)}$ and $\bstep{\ra}_{(i,\alpha)}$.

First, we prove two key lemmas for $\bstep{\ra}$: one for removing an
intermediate step, and another for reordering adjacent steps.
\begin{lemma}[Step removal]
\label{lem:drf-key-remove}
Suppose we have a release-acquire execution 
\[\mconf \bstep{\ra}_{(i_1,\alpha_1)} \mconf_1 \bstep{\ra}_{(i_2,\alpha_2)}
\cdots \bstep{\ra}_{(i_n,\alpha_n)} \mconf_n \]
such that
\[
\forall k \ge 2.~ \mconf_k.\mcths(i_k).\lview  \not\geq \mconf_1.\mcths(i_1).\lview ~.
\]
Then we have $i_k \neq i_1$ for all $k \ge 2$ and the following execution
\[
\mconf \bstep{\ra}_{(i_2,\alpha_2)} \mconf'_2 \bstep{\ra}_{(i_3,\alpha_3)}
\cdots \bstep{\ra}_{(i_n,\alpha_n)} \mconf'_n
\]
for some machine states $\mconf'_k$ satisfying
\[
\forall k\ge 2.~\forall i \neq i_1.~ \mconf'_k.\mcths(i).\lstate = \mconf_k.\mcths(i).\lstate~.
\]
\end{lemma}
\begin{proof}
There are only two cases where the first step with
$(i_1,\alpha_1)$ affects a subsequent step with $(i_k,\alpha_k)$:
either $(i)$ the latter reads what the former wrote; or $(ii)$ the
former globally synchronizes.  In case $(i)$, the view
$\mconf_k.\mcths(i_k).\lview$ becomes as high as the view
$\mconf_1.\mcths(i_1).\lview$ because the read and write are
$\ra$-synchronized. This is impossible because it conflicts with the
assumption.  In case $(ii)$, the effect is limited: $\mconf'_k$ is the
same as $\mconf_k$ except the effect of the event $\alpha_1$.  More
specifically, $\mconf_k$'s memory may contain an extra message
produced by $\alpha_1$ and the threads other than $i_1$ in $\mconf'_k$
are the same as those in $\mconf_k$ except that every view in the
former may be less than the corresponding view in the latter.  By
monotonicity, $\mconf'_k$ has more behaviors than $\mconf_k$ and thus
we can construct such an execution.
\end{proof}

\begin{lemma}[Step reorder]
\label{lem:drf-key-reorder}
Suppose we have a release-acquire execution 
\[\mconf \bstep{\ra}_{(i_1,\alpha_1)} \mconf_1 \bstep{\ra}_{(i_2,\alpha_2)} \mconf_2 \]
such that one of $\alpha_1$ and $\alpha_2$ is not globally synchronizing and
\[
\mconf_1.\mcths(i_1).\lview \not\leq \mconf_2.\mcths(i_2).\lview ~.
\]
Then we have $i_1 \neq i_2$ and $\mconf_1'$ satisfying
\[
\mconf \bstep{\ra}_{(i_2,\alpha_2)} \mconf'_1 \bstep{\ra}_{(i_1,\alpha_1)} \mconf_2 ~.
\]
\end{lemma}
\begin{proof}
Basically a similar argument as in the previous lemma applies here:
$(i)$ $\alpha_2$ should not read $\alpha_1$; and $(ii)$ the earlier
step with $\alpha_2$ does not affect the later step with
$\alpha_1$ since $\alpha_1$ or $\alpha_2$ is not globally synchronizing.
\end{proof}

Now we prove DRF-RA.
Let $\bstep{\ra}$ be identical to $\bstep{}$ in \cref{thm:drfpf}, except that
$(i)$ $\rlx$ and $\pln$ accesses in program transitions are interpreted as if they are all $\ra$-accesses, and
$(ii)$ a machine step consists only of one thread step.
Note that the second condition does not affect the semantics, since a machine state without promises is vacuously consistent.
% (that is, \textsc{read-helper} are \textsc{write-helper} are invoked with $\max\{o,\ra\}$ instead of $o$).

\begin{proof}[Proof of DRF-RA (\cref{thm:drfra})]
  It suffices to show that $(i)$ the existence of a $\rlx$-race in the
  $\bstep{}$-machine implies that in the $\bstep{\ra}$-machine, and
  $(ii)$ the behavior in the $\bstep{\ra}$-machine and that in the
  $\bstep{}$-machine coincide if $\prog$ is $\rlx$-race-free in the
  $\bstep{\ra}$-machine.  Then \cref{thm:drfpf} concludes the proof.

  We prove both $(i)$ and $(ii)$ by a single simulation argument.  We
  say an $\bstep{\ra}$-execution
\[\mconf'_0 \bstep{\ra}_{(i_1,\alpha_1)} \mconf'_1 \bstep{\ra}_{(i_2,\alpha_2)}
\cdots \bstep{\ra}_{(i_n,\alpha_n)} \mconf'_n \]
simulates a $\bstep{}$-execution
\[\mconf_0 \bstep{}_{(i_1,\alpha_1)} \mconf_1 \bstep{}_{(i_2,\alpha_2)}
\cdots \bstep{}_{(i_n,\alpha_n)} \mconf_n ~,\]
if the following conditions hold:
\begin{enumerate}
\item $\forall k, j.~\mconf_k.\mcths(j).\lstate = \mconf'_k.\mcths(j).\lstate~;$
\item $\forall k, j.~\mconf_k.\mcths(j).\lview.\lcur = \mconf'_k.\mcths(j).\lview.\lcur~;$
\item $\forall k, j.~\mconf_k.\mcths(j).\lview.\lacq = \mconf'_k.\mcths(j).\lview.\lacq~;$
\item $\forall k.~\mconf_k.\mcgsc = \mconf'_k.\mcgsc~;$ and
\item $\forall k.~\mconf_k.\mcmem$ and $\mconf'_k.\mcmem$ have the
  same messages, except that the released view of a message in the
  $\bstep{\ra}$-execution may be higher than that of the corresponding
  message in the $\bstep{}$-execution.
\end{enumerate}
This simulation proves $(i)$, as a $\rlx$-race in $\mconf_k$ in the
$\bstep{}$-execution is also a $\rlx$-race in $\mconf'_k$ in the
$\bstep{\ra}$-machine, thanks to the condition 1.  It also proves
$(ii)$ by the adequacy of the simulation relation.

Now we prove the simulation.  Consider a $\bstep{}$-step:
\[\mconf_n \bstep{}_{(i_{n+1},\alpha_{n+1})} \mconf_{n+1}~, \]
and we will find a corresponding $\bstep{\ra}$-step that preserves the
simulation relation:
\[\mconf'_n \bstep{\ra}_{(i_{n+1},\alpha_{n+1})} \mconf'_{n+1}~. \]
Thanks to the simulation relation, there exists $\mconf'_{n+1}$ such
that $\mconf'_n \bstep{\ra}_{(i_{n+1},\alpha_{n+1})} \mconf'_{n+1}$.
If $\alpha_{n+1}$ is not reading (\ie neither a read nor an update
event), it is immediate from the semantics that the simulation
relation is preserved.  Now suppose $\alpha_{n+1}$ is reading
$\twomsg{x}{t}$ with the access mode $o_r$, and let $k$ be such an
index that $\alpha_k$ is writing $\twomsg{x}{t}$ with the access mode
$o_w$, and $R$ (and $R_\ra$) be the released view of $\twomsg{x}{t}$ in
the $\bstep{}$-execution (and $\bstep{\ra}$-execution, respectively).

Now we proceed by a case analysis:
\begin{description}
\item [Case $o_w, o_r \sqsupseteq \ra$.]\ 

  Note that the current \& acquire views of
  $\mconf_{n+1}.\mcths(i_{n+1})$ and $\mconf'_{n+1}.\mcths(i_{n+1})$
  may diverge only due to the discrepancy of the $\twomsg{x}{t}$'s
  released views ($R$ and $R_\ra$), and the read's access mode ($o_r$
  for the $\bstep{}$-machine, and $o_r \sqcup \ra$ for the
  $\bstep{\ra}$-machine).  A similar argument applies to the other
  machine state components in the simulation relation.  Hence it
  suffices to show that $R=R_\ra$ and $o_r = o_r \sqcup \ra$, which
  come clearly from the assumption: in particular we have
  $R = \mconf_{k}.\mcths(i_{k}).\lview.\lcur =
  \mconf'_{k}.\mcths(i_{k}).\lview.\lcur = R_\ra$ thanks to
  $o_w \sqsupseteq \ra$.

\item [Case
  $\mconf'_{k}.\mcths(i_k).\lview.\lcur \leq
  \mconf'_{n}.\mcths(i_n).\lview.\lcur$.]\ 

  Since the released view
  $R_\ra = \mconf'_{k}.\mcths(i_k).\lview.\lcur$ of $\twomsg{x}{t}$ is
  already incorporated in the current view, a similar argument also
  applies here.

\item [Otherwise.]\ 

  In this case, we construct a $\rlx$-race in the
  $\bstep{\ra}$-execution by repeatedly applying the step-removing
  \cref{lem:drf-key-remove} to the execution:
\[\mconf'_{k-1} \bstep{\ra}_{(i_k,\alpha_k)} \mconf'_{k} \bstep{\ra}_{(i_{k+1},\alpha_{k+1})}
  \cdots \bstep{\ra}_{(i_n,\alpha_n)} \mconf'_n~, \] so that $i_k$
(attempting to write to $x$ with $o_w$) and $i_{n+1}$ (attempting to
read from $x$ with $o_r$) race in a reachable machine state.

Let $j \in [k,n)$ be the last such an index that
$\mconf'_{j}.\mcths(i_j).\lview.\lcur$
$\not\leq \mconf'_{n}.\mcths(i_n).\lview.\lcur$.  By
\cref{lem:drf-key-remove} there exists an $\bstep{\ra}$-execution:
\[
\mconf'_{j-1} \bstep{\ra}_{(i_{j+1},\alpha_{j+1})} \mconf''_{j+1}
\cdots \bstep{\ra}_{(i_n,\alpha_n)} \mconf''_n~,
\]
% \[
% \mconf'_{j-1} \bstep{\ra}_{(i_{j+1},\alpha_{j+1})} \mconf''_{j+1} \bstep{\ra}_{(i_{j+2},\alpha_{j+2})}
% \cdots \bstep{\ra}_{(i_n,\alpha_n)} \mconf''_n~,
% \]
such that
$\mconf''_n.\mcths(i_{n+1}).\lstate = \mconf'_n.\mcths(i_{n+1}).\lstate$.  By
repetition, we have an $\bstep{\ra}$-execution:
\[
  \mconf'_{k-1} \bstep{\ra}_{(i_{l},\alpha_{l})} \mconf'''_{l}
  \bstep{\ra}_{(i_{u},\alpha_{u})} \cdots
  \bstep{\ra}_{(i_n,\alpha_n)} \mconf'''_n~,
\]
such that
$\mconf'''_n.\mcths(i_{n+1}).\lstate =
\mconf'_n.\mcths(i_{n+1}).\lstate$ and $i_k$ is not executed at all
from $\mconf'_{k-1}$ to $\mconf'''_n$.  Hence
$\mconf'''_n.\mcths(i_k).\lstate = \mconf'_{k-1}.\mcths(i_k).\lstate$,
thus $i_k$ and $i_{n+1}$ race in $\mconf'''_n$.  
\end{description}
\end{proof}


\subsection{Proof of DRF-LOCK}
\label{sec:relaxed:proofs:drflock}

To state a DRF theorem on properly locked programs, we classify
locations into normal locations and \emph{lock locations} and suppose
lock locations are accessed only by the acquire and release operations, as
defined as follows:
$$\inparaII{
\textrm{acquire}(l)\ \{ \\
\quad \while{!\cas{l}{0}{1}{\acqr\relw}}{\skipc;} \\
\}
}{
\textrm{release}(l)\ \{ \\
\quad l_{\relw} := 0; \\
\}
}
$$

Furthermore, we say a machine state $\mconf$
is \emph{properly locked}, if:
\begin{enumerate}
\item If two different threads can take a step
  accessing the same location, then both accesses are reads, or the
  location is a lock location; and
\item If a thread can release a lock, say $l$, then the value
  of $l$ in $\mconf$'s memory is $1$.
\end{enumerate}

\begin{theorem}[DRF-LOCK]
\label{thm:drflock'}
Let $\bstep{\sco}$ denote the steps of the interleaving machine.
Suppose that every machine state that is $\bstep{\sco}$-reachable from
the initial state of a program $\prog$ is properly locked.  Then, the
behaviors of $\prog$ according to the full machine coincide with those
according to the $\bstep{\sco}$-machine.
\end{theorem}
\begin{proof}

We say that an $\bstep{\ra}$-execution is \emph{interleaving} if any
reading step reads from the message with the greatest timestamp and
any writing step writes a message with a timestamp greater than any
existing message's timestamp.
%% except for a failed CAS of a lock acquire operation.
It is obvious that the $\bstep{\sco}$-machine is equivalent to the
interleaving $\bstep{\ra}$-machine (which we simply call the
interleaving machine), and thus we identify them.
%% modulo the failed attempts to acquire a lock.  
%% Thus we identify the interleaving $\bstep{\ra}$-executions with
%% the interleaving execution.
%% Thus it suffices to show the DRF theorem for the interleaving
%% machine.

First of all, for any $\bstep{\ra}$-execution $E$ (\ie a finite or
infinite sequence of $\bstep{\ra}$-steps), it is easy to see that
removing all failed acquire steps from the execution still yields a
valid $\bstep{\ra}$-execution $E'$ with the same behavior (\ie the
same sequence of system calls). Furthermore, if $E$ is a finite
execution leading to a $\ra$-racy machine state, then so is $E'$.  We
will simply say $\bstep{\nf\ra}$-executions for $\bstep{\ra}$-executions
with no failed acquire steps.

From this observation and \cref{thm:drfra}, we can easily see that
it suffices to prove that $(i)$ the existence of a $\ra$-race in an
$\bstep{\nf\ra}$-execution of $\prog$ implies a violation of proper
locking in an interleaving execution of $\prog$; and $(ii)$ the
$\bstep{\nf\ra}$-behaviors of $\prog$ coincide with its interleaving
behaviors if $\prog$ is properly locked in all interleaving executions.

We prove both $(i)$ and $(ii)$ by a single simulation argument.  We
say an interleaving execution
\[\mconf'_0 \bstep{\ra}_{(i'_1,\alpha'_1)} \mconf'_1 \bstep{\ra}_{(i'_2,\alpha'_2)}
  \cdots \bstep{\ra}_{(i'_n,\alpha'_n)} \mconf'_n \] simulates an
$\bstep{\nf\ra}$-execution
\[\mconf_0 \bstep{\ra}_{(i_1,\alpha_1)} \mconf_1 \bstep{\ra}_{(i_2,\alpha_2)}
  \cdots \bstep{\ra}_{(i_n,\alpha_n)} \mconf_n ~,\] if the following
conditions hold:
\begin{enumerate}
\item $(i'_1,\alpha'_1),\ldots,(i'_n,\alpha'_n)$ is a reordering of
  $(i_1,\alpha_1),\ldots,(i_n,\alpha_n)$ such that the order of
%% globally synchronizing events, including 
  system calls is preserved; and
\item $\mconf_0  = \mconf'_0$ and $\mconf'_n = \mconf_n$. 
%% and
%% \item For all $i$ and lock location $l$, $l$ is well-formed in $\mconf'_i.\mcmem~.$
\end{enumerate}

%% Here, we say a lock location $l$
%% is \emph{well-formed} in a memory $m$ if:
%% \begin{enumerate}
%% \item For every message $\updmsg{l}{v}{f}{t}$ in $m(l)$, $v$ is either $0$ or $1$;
%% \item For any adjacent messages $\updmsg{l}{v_1}{f_1}{t_1}$ and $\updmsg{l}{v_2}{f_2}{t_2}$ in $m(l)$, 
%% if $v_1=0$, we have $v_2=1$ and $f_2=t_1$; otherwise, we have $v_2=0$.

%%  and
%% \item For adjacent messages $\updmsg{l}{1}{f_1}{t_1}$
%%   and $\updmsg{l}{v_2}{f_2}{t_2}$ in $m(l)$, $v_2=0$ and 
%% \end{enumerate}

If we prove that given any $\bstep{\nf\ra}$-execution of length $n$
there exists a simulating interleaving execution, then we are done as
follows.  Given any (possibly infinite) $\bstep{\nf\ra}$-execution and
any number of steps $n$, we can find an interleaving execution of
length $n$ leading to the same machine state with the same sequence of
observable events (\ie system calls). Thus, any arbitrarily long
observation on an $\bstep{\nf\ra}$-execution cannot be distinguished
from that on an interleaving execution. Also, if there is any
$\bstep{\nf\ra}$-execution leading to a $\ra$-racy machine state, we
can find a simulating interleaving execution to an improperly locked
machine state by the simulation argument.

Now it suffices to prove the simulation theorem by induction on the
length $n$.  The base case is trivial. For an induction step, let's
assume that we have a simulating execution of length $n$, given as in
the above definition of simulation. Suppose we have a step $\mconf_n
\bstep{\nf\ra}_{i_{n+1},\alpha_{n+1}} \mconf_{n+1}$.  Then we need to
find a simulating interleaving execution of length $n+1$ that starts
from $\mconf_0$ and ending in $\mconf_{n+1}$. If the event
$\alpha_{n+1}$ is neither a read, a write, nor an update, then the
execution $\mconf'_0 \ldots \mconf'_n \bstep{\ra}_{(i_{n+1},)}
\mconf_{n+1}$ is interleaving, so we are done.

Thus suppose that $\alpha_{n+1}$ is accessing (\ie a read, a write, or
an update event on) a location $x$ and does not satisfy the
interleaving condition (\ie does not read the latest message nor
writes with a greatest timestamp).
%% , and $\alpha_{n+1}$ is not a failed CAS of a lock location
By definition of the interleaving
condition, we can find an event writing to $x$ whose timestamp is
bigger than that of the event $\alpha_{n+1}$. Let's write $\alpha_k$
and $t_k$ for the first such event (\ie with the smallest index $k$)
and its timestamp.

Now, by exactly the same argument as in \cref{thm:drfra}, we can
remove all steps $\mconf_j$ such that $k \le j\le n$ and
$\mconf_j.\mcths(i_j).\lview \ge \mconf_k.\mcths(i_k).\lview$.  Then
we have a race between $\alpha_k$ and $\alpha_{n+1}$.  The resulting
execution is also interleaving because removing a step from an
interleaving execution always results in an interleaving execution.
Thus, by the proper locking assumption, it follows that
$\alpha_k$ and $\alpha_{n+1}$ are accessing the same lock location.

Now we will construct an interleaving execution from $\mconf_0$ to
$\mconf_{n+1}$ that simulates the given execution. For this, we
repeatedly apply the step-reordering using
\cref{lem:drf-key-reorder} as follows. First, we find the first
event, say $\alpha_j$, such that $k \le j \le n+1$ and
$\mconf_j.\mcths(i_j).\lview.\lcur.\lrw(x) < t_k$.  Then we can move
down the event to just before $\alpha_{k}$ using
\cref{lem:drf-key-reorder}.  We repeat this process until we move
$\alpha_{n+1}$ down to just before $\alpha_k$.  This is possible
because we have ${\mconf_{n+1}.\mcths(i_{n+1}).\lview.\lcur.\lrw(x) <
  t_k}$. Also note that this process does not reorder system calls
because we assume that system calls synchronize on lock
locations (\ie making the view on lock locations to be up-to date).

Finally we will show that such a reordering does not break the
interleaving condition for all existing events and furthermore make
$\alpha_{n+1}$ to satisfy the interleaving condition.  The latter
holds trivially by construction because $\alpha_k$ was the first event
with respect to which $\alpha_{n+1}$ violates the interleaving
condition. The former holds as follows. In order to break the
interleaving condition, we need to reorder two events $\alpha,\beta$
to $\beta,\alpha$ such that they are accessing the same location and
at least one of them is writing. During the reordering process,
suppose we meet such a reordering for the first time.  Then, the
execution before the reordering is interleaving because we are about
to break the condition for the first time.  Since $\alpha$ and $\beta$
are racing on the same location, they both have to be lock operations
(\ie successful acquire or release). By the proper locking assumption,
we have only two possibilities: $\alpha$ is a release and $\beta$ is an
acquire; or $\alpha$ is an acquire and $\beta$ is a release.  The
former case is a contradiction because $\beta$ reads what $\alpha$
writes due to the interleaving condition, which makes $\beta$'s view
as high as $\alpha$'s. The latter is a contradiction too because after
reordering the execution up to $\beta$ is still interleaving but the
machine state before $\beta$ is not properly locked. Thus we can
conclude that the reorderings do not break the interleaving condition.

%% First, we have that $\alpha_k$ is a release because otherwise it
%% contradicts the fact that $M_{n}$ is well-formed and $\alpha_k$ is the
%% first event whose timestamp is bigger than $\alpha_{n+1}$'s.

%% Also, $\alpha_{n+1}$ should
%% not be releasing.  If so, $\alpha_{n+1}$ is filling a gap between
%%  $\alpha_k$ and the writing event to $x$ just before $\alpha_k$ in
%% the timestamp order, and by well-formedness its value should be $0$.
%% It contradicts with the assumption that $\mconf'_n$ is properly
%% locked.

%% Now suppose that $x$ is a lock location, and $\alpha_{n+1}$ is
%% acquiring it.  Since it is not a failed CAS of a lock location,
%% $\alpha_{n+1}$ should be a successful CAS.  However it contradicts
%% with the well-formedness assumption because for a CAS to be
%% successful, its message should be the last one in the memory.


\end{proof}



%\begin{theorem}[DRF-SC]
%\label{thm:drfsc}
%Let $\bstep{\sco}$ denote the steps of the interleaving machine.
%Suppose that every machine state that is $\bstep{\sco}$-reachable from the initial state of a program $\prog$ is $\ra$-race-free.
%Then, the behaviors of $\prog$ according to the full machine 
%coincide with those according to the $\bstep{\sco}$-machine.
%\end{theorem}

%\textbf{Our proof of \Cref{thm:drfpf} is fully mechanized in Coq.} 

%\ori{I'm glossing over some problem here: the standard result also has locks.
%anyway if a reviewer complains, we can explain how this can be extended to locks}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
